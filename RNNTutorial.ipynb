{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "np.set_printoptions(precision=4, linewidth=200)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.reader import ptb_raw_data\n",
    "from utils.batcher import ptb_batcher\n",
    "from utils.conditional_scope import cond_name_scope, cond_variable_scope\n",
    "from utils.unrolled_rnn import make_rnn_variables\n",
    "from utils.unrolled_rnn import make_rnn_outputs\n",
    "from utils.unrolled_rnn import make_summary_nodes\n",
    "from utils.unrolled_rnn import make_placeholders\n",
    "from utils.unrolled_rnn import make_train_op\n",
    "from utils.batcher import generate_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, vocab_size = ptb_raw_data('bigdata/simple-examples/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE=64\n",
    "HIDDEN_SIZE=256\n",
    "BATCH_SIZE=32\n",
    "NUM_STEPS=16\n",
    "NUM_EPOCHS_INIT_LR=3\n",
    "NUM_EPOCHS_TOTAL=8\n",
    "INITIAL_LR=5e0\n",
    "LR_DECAY_RATE=0.75\n",
    "MAX_NORM=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "placeholders = make_placeholders(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_steps=NUM_STEPS\n",
    ")\n",
    "rnn_vars = make_rnn_variables(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    initializer_scale=0.2\n",
    ")\n",
    "rnn_outputs = make_rnn_outputs(\n",
    "    input_sequence=placeholders['inputs'],\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_steps=NUM_STEPS,\n",
    "    rnn_variables=rnn_vars\n",
    ")\n",
    "summary_nodes = make_summary_nodes(\n",
    "    targets=placeholders['targets'],\n",
    "    logits=rnn_outputs['logits'],\n",
    ")\n",
    "training_nodes = make_train_op(\n",
    "    summary_nodes['loss'],\n",
    "    placeholders['learning_rate'],\n",
    "    placeholders['max_norm'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 64    loss: 8.265625    gradient norm: 3.865234375     correct words: 37\n",
      "step: 128    loss: 7.16015625    gradient norm: 0.7783203125     correct words: 46\n",
      "step: 192    loss: 7.6953125    gradient norm: 3.443359375     correct words: 32\n",
      "step: 256    loss: 6.875    gradient norm: 0.765625     correct words: 34\n",
      "step: 320    loss: 6.3828125    gradient norm: 1.2119140625     correct words: 53\n",
      "step: 384    loss: 6.34765625    gradient norm: 1.0400390625     correct words: 70\n",
      "step: 448    loss: 6.36328125    gradient norm: 0.50537109375     correct words: 51\n",
      "step: 512    loss: 6.7734375    gradient norm: 1.5263671875     correct words: 54\n",
      "step: 576    loss: 6.4921875    gradient norm: 1.5439453125     correct words: 60\n",
      "step: 640    loss: 6.0546875    gradient norm: 0.642578125     correct words: 63\n",
      "step: 704    loss: 6.078125    gradient norm: 0.55126953125     correct words: 59\n",
      "step: 768    loss: 6.2734375    gradient norm: 0.6455078125     correct words: 50\n",
      "step: 832    loss: 6.09765625    gradient norm: 0.60693359375     correct words: 68\n",
      "step: 896    loss: 5.8359375    gradient norm: 1.009765625     correct words: 71\n",
      "step: 960    loss: 5.984375    gradient norm: 1.4912109375     correct words: 73\n",
      "step: 1024    loss: 6.1484375    gradient norm: 0.51171875     correct words: 64\n",
      "step: 1088    loss: 5.9921875    gradient norm: 0.9658203125     correct words: 78\n",
      "step: 1152    loss: 6.2265625    gradient norm: 1.2978515625     correct words: 73\n",
      "step: 1216    loss: 5.6640625    gradient norm: 0.72998046875     correct words: 69\n",
      "step: 1280    loss: 5.6796875    gradient norm: 0.9453125     correct words: 76\n",
      "step: 1344    loss: 5.62109375    gradient norm: 0.66796875     correct words: 87\n",
      "step: 1408    loss: 5.9765625    gradient norm: 0.6552734375     correct words: 69\n",
      "step: 1472    loss: 6.0390625    gradient norm: 0.6318359375     correct words: 62\n",
      "step: 1536    loss: 5.94140625    gradient norm: 0.55859375     correct words: 60\n",
      "step: 1600    loss: 5.734375    gradient norm: 0.485595703125     correct words: 94\n",
      "step: 1664    loss: 6.1953125    gradient norm: 1.7568359375     correct words: 83\n",
      "step: 1728    loss: 5.76171875    gradient norm: 0.4775390625     correct words: 93\n",
      "step: 1792    loss: 5.72265625    gradient norm: 0.58203125     correct words: 75\n",
      "validation perplexity: 329.841128949\n",
      "test perplexity: 325.997479075\n",
      "step: 64    loss: 5.9296875    gradient norm: 1.1845703125     correct words: 76\n",
      "step: 128    loss: 5.9296875    gradient norm: 0.6025390625     correct words: 73\n",
      "step: 192    loss: 5.8515625    gradient norm: 0.5361328125     correct words: 74\n",
      "step: 256    loss: 5.953125    gradient norm: 0.71826171875     correct words: 72\n",
      "step: 320    loss: 5.390625    gradient norm: 0.9072265625     correct words: 90\n",
      "step: 384    loss: 5.63671875    gradient norm: 0.5322265625     correct words: 86\n",
      "step: 448    loss: 5.7109375    gradient norm: 0.6337890625     correct words: 77\n",
      "step: 512    loss: 5.7421875    gradient norm: 0.95703125     correct words: 78\n",
      "step: 576    loss: 5.6328125    gradient norm: 0.65576171875     correct words: 83\n",
      "step: 640    loss: 5.3125    gradient norm: 0.76904296875     correct words: 97\n",
      "step: 704    loss: 5.4765625    gradient norm: 0.55126953125     correct words: 87\n",
      "step: 768    loss: 5.609375    gradient norm: 0.5869140625     correct words: 64\n",
      "step: 832    loss: 5.4765625    gradient norm: 0.5107421875     correct words: 93\n",
      "step: 896    loss: 5.2109375    gradient norm: 0.75634765625     correct words: 86\n",
      "step: 960    loss: 5.3671875    gradient norm: 0.52978515625     correct words: 93\n",
      "step: 1024    loss: 5.68359375    gradient norm: 0.78173828125     correct words: 87\n",
      "step: 1088    loss: 5.6484375    gradient norm: 1.4794921875     correct words: 92\n",
      "step: 1152    loss: 5.6796875    gradient norm: 0.5234375     correct words: 88\n",
      "step: 1216    loss: 5.109375    gradient norm: 0.56298828125     correct words: 110\n",
      "step: 1280    loss: 5.2578125    gradient norm: 0.57861328125     correct words: 101\n",
      "step: 1344    loss: 5.1171875    gradient norm: 0.51416015625     correct words: 86\n",
      "step: 1408    loss: 5.40625    gradient norm: 0.60107421875     correct words: 92\n",
      "step: 1472    loss: 5.5390625    gradient norm: 0.55615234375     correct words: 102\n",
      "step: 1536    loss: 5.5    gradient norm: 0.5537109375     correct words: 88\n",
      "step: 1600    loss: 5.3984375    gradient norm: 0.52685546875     correct words: 100\n",
      "step: 1664    loss: 5.39453125    gradient norm: 0.50390625     correct words: 101\n",
      "step: 1728    loss: 5.3828125    gradient norm: 1.0478515625     correct words: 98\n",
      "step: 1792    loss: 5.42578125    gradient norm: 0.92431640625     correct words: 83\n",
      "validation perplexity: 232.96156454\n",
      "test perplexity: 221.725566381\n",
      "step: 64    loss: 5.5234375    gradient norm: 0.5615234375     correct words: 91\n",
      "step: 128    loss: 5.59765625    gradient norm: 0.572265625     correct words: 90\n",
      "step: 192    loss: 5.4375    gradient norm: 0.5791015625     correct words: 97\n",
      "step: 256    loss: 5.33984375    gradient norm: 0.54541015625     correct words: 98\n",
      "step: 320    loss: 5.2578125    gradient norm: 1.1630859375     correct words: 106\n",
      "step: 384    loss: 5.27734375    gradient norm: 0.51904296875     correct words: 104\n",
      "step: 448    loss: 5.90625    gradient norm: 1.6142578125     correct words: 74\n",
      "step: 512    loss: 5.3984375    gradient norm: 0.82080078125     correct words: 88\n",
      "step: 576    loss: 5.28125    gradient norm: 0.63671875     correct words: 95\n",
      "step: 640    loss: 5.2421875    gradient norm: 0.62890625     correct words: 86\n",
      "step: 704    loss: 5.265625    gradient norm: 0.58984375     correct words: 91\n",
      "step: 768    loss: 5.3515625    gradient norm: 1.2197265625     correct words: 91\n",
      "step: 832    loss: 5.234375    gradient norm: 1.0830078125     correct words: 102\n",
      "step: 896    loss: 4.9609375    gradient norm: 0.66259765625     correct words: 90\n",
      "step: 960    loss: 5.12109375    gradient norm: 0.75439453125     correct words: 103\n",
      "step: 1024    loss: 5.703125    gradient norm: 1.4326171875     correct words: 87\n",
      "step: 1088    loss: 5.1796875    gradient norm: 0.73193359375     correct words: 118\n",
      "step: 1152    loss: 5.46484375    gradient norm: 0.595703125     correct words: 95\n",
      "step: 1216    loss: 5.1328125    gradient norm: 0.64013671875     correct words: 97\n",
      "step: 1280    loss: 5.1328125    gradient norm: 1.2314453125     correct words: 97\n",
      "step: 1344    loss: 4.96875    gradient norm: 0.5986328125     correct words: 97\n",
      "step: 1408    loss: 5.1640625    gradient norm: 0.638671875     correct words: 102\n",
      "step: 1472    loss: 5.09375    gradient norm: 0.5390625     correct words: 114\n",
      "step: 1536    loss: 5.1875    gradient norm: 0.55859375     correct words: 104\n",
      "step: 1600    loss: 5.171875    gradient norm: 1.046875     correct words: 110\n",
      "step: 1664    loss: 5.0625    gradient norm: 0.84423828125     correct words: 112\n",
      "step: 1728    loss: 5.3359375    gradient norm: 0.556640625     correct words: 103\n",
      "step: 1792    loss: 5.2109375    gradient norm: 0.6103515625     correct words: 100\n",
      "validation perplexity: 239.946143608\n",
      "test perplexity: 236.198570475\n",
      "step: 64    loss: 5.171875    gradient norm: 0.5166015625     correct words: 94\n",
      "step: 128    loss: 5.203125    gradient norm: 0.52197265625     correct words: 103\n",
      "step: 192    loss: 5.015625    gradient norm: 0.6865234375     correct words: 99\n",
      "step: 256    loss: 4.921875    gradient norm: 0.5361328125     correct words: 105\n",
      "step: 320    loss: 4.828125    gradient norm: 0.6416015625     correct words: 122\n",
      "step: 384    loss: 4.8359375    gradient norm: 0.5322265625     correct words: 117\n",
      "step: 448    loss: 5.3046875    gradient norm: 0.7958984375     correct words: 72\n",
      "step: 512    loss: 5.0390625    gradient norm: 0.52197265625     correct words: 100\n",
      "step: 576    loss: 4.8359375    gradient norm: 0.63427734375     correct words: 122\n",
      "step: 640    loss: 4.80078125    gradient norm: 0.6875     correct words: 115\n",
      "step: 704    loss: 4.81640625    gradient norm: 0.693359375     correct words: 102\n",
      "step: 768    loss: 4.8984375    gradient norm: 0.7080078125     correct words: 105\n",
      "step: 832    loss: 4.859375    gradient norm: 0.5634765625     correct words: 112\n",
      "step: 896    loss: 4.76953125    gradient norm: 0.6923828125     correct words: 111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 960    loss: 4.7421875    gradient norm: 0.53515625     correct words: 116\n",
      "step: 1024    loss: 5.27734375    gradient norm: 1.03125     correct words: 109\n",
      "step: 1088    loss: 5.03125    gradient norm: 0.57666015625     correct words: 112\n",
      "step: 1152    loss: 5.03515625    gradient norm: 0.54296875     correct words: 105\n",
      "step: 1216    loss: 4.62109375    gradient norm: 0.58251953125     correct words: 124\n",
      "step: 1280    loss: 4.7734375    gradient norm: 0.5849609375     correct words: 109\n",
      "step: 1344    loss: 4.6796875    gradient norm: 0.5576171875     correct words: 114\n",
      "step: 1408    loss: 4.7890625    gradient norm: 0.56298828125     correct words: 110\n",
      "step: 1472    loss: 4.859375    gradient norm: 0.544921875     correct words: 123\n",
      "step: 1536    loss: 4.9375    gradient norm: 0.89990234375     correct words: 106\n",
      "step: 1600    loss: 4.828125    gradient norm: 0.52978515625     correct words: 124\n",
      "step: 1664    loss: 4.8515625    gradient norm: 0.55859375     correct words: 118\n",
      "step: 1728    loss: 5.0546875    gradient norm: 0.62451171875     correct words: 99\n",
      "step: 1792    loss: 4.90625    gradient norm: 0.60546875     correct words: 100\n",
      "validation perplexity: 181.46510351\n",
      "test perplexity: 171.956443785\n",
      "step: 64    loss: 4.98828125    gradient norm: 0.63525390625     correct words: 103\n",
      "step: 128    loss: 5.04296875    gradient norm: 0.703125     correct words: 110\n",
      "step: 192    loss: 4.7890625    gradient norm: 0.5810546875     correct words: 109\n",
      "step: 256    loss: 4.74609375    gradient norm: 0.59326171875     correct words: 107\n",
      "step: 320    loss: 4.6875    gradient norm: 0.662109375     correct words: 128\n",
      "step: 384    loss: 4.65234375    gradient norm: 0.49609375     correct words: 122\n",
      "step: 448    loss: 5.05078125    gradient norm: 0.521484375     correct words: 90\n",
      "step: 512    loss: 4.859375    gradient norm: 0.53271484375     correct words: 109\n",
      "step: 576    loss: 4.66015625    gradient norm: 0.56005859375     correct words: 130\n",
      "step: 640    loss: 4.609375    gradient norm: 0.5693359375     correct words: 120\n",
      "step: 704    loss: 4.60546875    gradient norm: 0.52197265625     correct words: 113\n",
      "step: 768    loss: 4.73046875    gradient norm: 0.591796875     correct words: 113\n",
      "step: 832    loss: 4.5546875    gradient norm: 0.537109375     correct words: 122\n",
      "step: 896    loss: 4.6015625    gradient norm: 0.560546875     correct words: 125\n",
      "step: 960    loss: 4.5703125    gradient norm: 0.5751953125     correct words: 116\n",
      "step: 1024    loss: 5.1328125    gradient norm: 0.9609375     correct words: 120\n",
      "step: 1088    loss: 4.828125    gradient norm: 0.6416015625     correct words: 112\n",
      "step: 1152    loss: 4.91796875    gradient norm: 0.82373046875     correct words: 119\n",
      "step: 1216    loss: 4.46484375    gradient norm: 0.59765625     correct words: 131\n",
      "step: 1280    loss: 4.546875    gradient norm: 0.599609375     correct words: 128\n",
      "step: 1344    loss: 4.58984375    gradient norm: 0.59228515625     correct words: 101\n",
      "step: 1408    loss: 4.5390625    gradient norm: 0.57177734375     correct words: 117\n",
      "step: 1472    loss: 4.734375    gradient norm: 0.5830078125     correct words: 133\n",
      "step: 1536    loss: 4.72265625    gradient norm: 0.5595703125     correct words: 108\n",
      "step: 1600    loss: 4.6015625    gradient norm: 0.52978515625     correct words: 137\n",
      "step: 1664    loss: 4.625    gradient norm: 0.6298828125     correct words: 130\n",
      "step: 1728    loss: 4.7265625    gradient norm: 0.53564453125     correct words: 117\n",
      "step: 1792    loss: 4.6875    gradient norm: 0.6328125     correct words: 105\n",
      "validation perplexity: 166.039100748\n",
      "test perplexity: 158.2899869\n",
      "step: 64    loss: 4.8671875    gradient norm: 0.55517578125     correct words: 106\n",
      "step: 128    loss: 4.99609375    gradient norm: 0.8037109375     correct words: 113\n",
      "step: 192    loss: 4.63671875    gradient norm: 0.53564453125     correct words: 116\n",
      "step: 256    loss: 4.55859375    gradient norm: 0.55078125     correct words: 122\n",
      "step: 320    loss: 4.5703125    gradient norm: 0.609375     correct words: 136\n",
      "step: 384    loss: 4.546875    gradient norm: 0.51416015625     correct words: 125\n",
      "step: 448    loss: 4.8984375    gradient norm: 0.5576171875     correct words: 105\n",
      "step: 512    loss: 4.7265625    gradient norm: 0.53955078125     correct words: 115\n",
      "step: 576    loss: 4.5625    gradient norm: 0.546875     correct words: 128\n",
      "step: 640    loss: 4.5    gradient norm: 0.55224609375     correct words: 123\n",
      "step: 704    loss: 4.5234375    gradient norm: 0.5908203125     correct words: 122\n",
      "step: 768    loss: 4.62890625    gradient norm: 0.5693359375     correct words: 117\n",
      "step: 832    loss: 4.3984375    gradient norm: 0.5380859375     correct words: 128\n",
      "step: 896    loss: 4.5078125    gradient norm: 0.56005859375     correct words: 128\n",
      "step: 960    loss: 4.453125    gradient norm: 0.6025390625     correct words: 123\n",
      "step: 1024    loss: 4.99609375    gradient norm: 0.79541015625     correct words: 122\n",
      "step: 1088    loss: 4.6640625    gradient norm: 0.60400390625     correct words: 125\n",
      "step: 1152    loss: 4.7890625    gradient norm: 0.62744140625     correct words: 128\n",
      "step: 1216    loss: 4.3515625    gradient norm: 0.58984375     correct words: 136\n",
      "step: 1280    loss: 4.39453125    gradient norm: 0.59912109375     correct words: 134\n",
      "step: 1344    loss: 4.4453125    gradient norm: 0.587890625     correct words: 112\n",
      "step: 1408    loss: 4.3828125    gradient norm: 0.58544921875     correct words: 125\n",
      "step: 1472    loss: 4.6328125    gradient norm: 0.59130859375     correct words: 138\n",
      "step: 1536    loss: 4.59375    gradient norm: 0.57568359375     correct words: 119\n",
      "step: 1600    loss: 4.484375    gradient norm: 0.5703125     correct words: 143\n",
      "step: 1664    loss: 4.49609375    gradient norm: 0.5546875     correct words: 143\n",
      "step: 1728    loss: 4.6171875    gradient norm: 0.55517578125     correct words: 125\n",
      "step: 1792    loss: 4.5390625    gradient norm: 0.62158203125     correct words: 114\n",
      "validation perplexity: 161.017948802\n",
      "test perplexity: 152.639078694\n",
      "step: 64    loss: 4.77734375    gradient norm: 0.5556640625     correct words: 110\n",
      "step: 128    loss: 4.9140625    gradient norm: 0.68408203125     correct words: 112\n",
      "step: 192    loss: 4.55078125    gradient norm: 0.556640625     correct words: 117\n",
      "step: 256    loss: 4.46875    gradient norm: 0.5556640625     correct words: 123\n",
      "step: 320    loss: 4.484375    gradient norm: 0.57763671875     correct words: 141\n",
      "step: 384    loss: 4.48046875    gradient norm: 0.52734375     correct words: 129\n",
      "step: 448    loss: 4.83203125    gradient norm: 0.62451171875     correct words: 107\n",
      "step: 512    loss: 4.65234375    gradient norm: 0.56787109375     correct words: 114\n",
      "step: 576    loss: 4.49609375    gradient norm: 0.5703125     correct words: 129\n",
      "step: 640    loss: 4.4296875    gradient norm: 0.564453125     correct words: 129\n",
      "step: 704    loss: 4.4296875    gradient norm: 0.572265625     correct words: 135\n",
      "step: 768    loss: 4.578125    gradient norm: 0.5869140625     correct words: 122\n",
      "step: 832    loss: 4.30078125    gradient norm: 0.5478515625     correct words: 133\n",
      "step: 896    loss: 4.4453125    gradient norm: 0.56982421875     correct words: 135\n",
      "step: 960    loss: 4.359375    gradient norm: 0.61376953125     correct words: 135\n",
      "step: 1024    loss: 4.8984375    gradient norm: 0.6640625     correct words: 124\n",
      "step: 1088    loss: 4.5546875    gradient norm: 0.56494140625     correct words: 141\n",
      "step: 1152    loss: 4.72265625    gradient norm: 0.6181640625     correct words: 139\n",
      "step: 1216    loss: 4.2890625    gradient norm: 0.6064453125     correct words: 140\n",
      "step: 1280    loss: 4.30078125    gradient norm: 0.59716796875     correct words: 138\n",
      "step: 1344    loss: 4.359375    gradient norm: 0.59521484375     correct words: 122\n",
      "step: 1408    loss: 4.28125    gradient norm: 0.60107421875     correct words: 127\n",
      "step: 1472    loss: 4.5625    gradient norm: 0.60400390625     correct words: 140\n",
      "step: 1536    loss: 4.52734375    gradient norm: 0.591796875     correct words: 119\n",
      "step: 1600    loss: 4.3984375    gradient norm: 0.58935546875     correct words: 147\n",
      "step: 1664    loss: 4.421875    gradient norm: 0.58642578125     correct words: 142\n",
      "step: 1728    loss: 4.55859375    gradient norm: 0.5908203125     correct words: 132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1792    loss: 4.4609375    gradient norm: 0.6064453125     correct words: 120\n",
      "validation perplexity: 158.706612236\n",
      "test perplexity: 149.551617614\n",
      "step: 64    loss: 4.72265625    gradient norm: 0.56591796875     correct words: 115\n",
      "step: 128    loss: 4.8671875    gradient norm: 0.6513671875     correct words: 115\n",
      "step: 192    loss: 4.5078125    gradient norm: 0.58642578125     correct words: 118\n",
      "step: 256    loss: 4.4296875    gradient norm: 0.58349609375     correct words: 119\n",
      "step: 320    loss: 4.4296875    gradient norm: 0.578125     correct words: 138\n",
      "step: 384    loss: 4.44921875    gradient norm: 0.54052734375     correct words: 135\n",
      "step: 448    loss: 4.78125    gradient norm: 0.6318359375     correct words: 110\n",
      "step: 512    loss: 4.609375    gradient norm: 0.5869140625     correct words: 115\n",
      "step: 576    loss: 4.4609375    gradient norm: 0.5947265625     correct words: 134\n",
      "step: 640    loss: 4.390625    gradient norm: 0.5791015625     correct words: 130\n",
      "step: 704    loss: 4.3828125    gradient norm: 0.58154296875     correct words: 138\n",
      "step: 768    loss: 4.54296875    gradient norm: 0.60595703125     correct words: 128\n",
      "step: 832    loss: 4.25    gradient norm: 0.55908203125     correct words: 139\n",
      "step: 896    loss: 4.3984375    gradient norm: 0.57275390625     correct words: 132\n",
      "step: 960    loss: 4.29296875    gradient norm: 0.61181640625     correct words: 140\n",
      "step: 1024    loss: 4.828125    gradient norm: 0.62939453125     correct words: 131\n",
      "step: 1088    loss: 4.5    gradient norm: 0.5732421875     correct words: 146\n",
      "step: 1152    loss: 4.671875    gradient norm: 0.5986328125     correct words: 141\n",
      "step: 1216    loss: 4.2421875    gradient norm: 0.6044921875     correct words: 148\n",
      "step: 1280    loss: 4.25    gradient norm: 0.609375     correct words: 144\n",
      "step: 1344    loss: 4.30078125    gradient norm: 0.61181640625     correct words: 128\n",
      "step: 1408    loss: 4.2109375    gradient norm: 0.60546875     correct words: 138\n",
      "step: 1472    loss: 4.5234375    gradient norm: 0.6123046875     correct words: 143\n",
      "step: 1536    loss: 4.4921875    gradient norm: 0.5986328125     correct words: 126\n",
      "step: 1600    loss: 4.3515625    gradient norm: 0.58935546875     correct words: 149\n",
      "step: 1664    loss: 4.375    gradient norm: 0.6103515625     correct words: 148\n",
      "step: 1728    loss: 4.51171875    gradient norm: 0.61181640625     correct words: 136\n",
      "step: 1792    loss: 4.421875    gradient norm: 0.60693359375     correct words: 131\n",
      "validation perplexity: 159.422885665\n",
      "test perplexity: 149.496860203\n"
     ]
    }
   ],
   "source": [
    "training_outputs = {**summary_nodes, **training_nodes}\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Bookkeeping\n",
    "    run_id = time.time()\n",
    "    writer = tf.summary.FileWriter('logs/{0}'.format(run_id), sess.graph)\n",
    "    coord = tf.train.Coordinator()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    learning_rate = INITIAL_LR\n",
    "    max_norm = MAX_NORM\n",
    "    for i in range(NUM_EPOCHS_TOTAL):\n",
    "        if i >= NUM_EPOCHS_INIT_LR:\n",
    "            learning_rate *= LR_DECAY_RATE\n",
    "        for batch_idx, (inputs, targets) in enumerate(generate_epoch(X_train, BATCH_SIZE, NUM_STEPS)):\n",
    "            outputs = sess.run(\n",
    "                training_outputs,\n",
    "                feed_dict={\n",
    "                    placeholders['inputs']: inputs,\n",
    "                    placeholders['targets']: targets,\n",
    "                    placeholders['learning_rate']: learning_rate,\n",
    "                    placeholders['max_norm']: max_norm,\n",
    "                }\n",
    "            )\n",
    "            if (batch_idx % 64 == 63):\n",
    "                print('step: {0}    loss: {1}    gradient norm: {2}     correct words: {3}'.format(\n",
    "                    batch_idx+1,\n",
    "                    outputs['loss'],\n",
    "                    outputs['gradient_global_norm'],\n",
    "                    outputs['num_correct_predictions'],\n",
    "                ))\n",
    "                \n",
    "        total_loss, total_batches = 0, 0\n",
    "        for inputs, targets in generate_epoch(X_val, BATCH_SIZE, NUM_STEPS):\n",
    "            outputs = sess.run(\n",
    "                summary_nodes,\n",
    "                feed_dict={\n",
    "                    placeholders['inputs']: inputs,\n",
    "                    placeholders['targets']: targets\n",
    "                },\n",
    "            )\n",
    "            total_loss += outputs['loss']\n",
    "            total_batches += 1\n",
    "        print('validation perplexity:', np.exp(total_loss / total_batches))\n",
    "        total_loss, total_batches = 0, 0\n",
    "        for inputs, targets in generate_epoch(X_test, BATCH_SIZE, NUM_STEPS):\n",
    "            outputs = sess.run(\n",
    "                summary_nodes,\n",
    "                feed_dict={\n",
    "                    placeholders['inputs']: inputs,\n",
    "                    placeholders['targets']: targets\n",
    "                },\n",
    "            )\n",
    "            total_loss += outputs['loss']\n",
    "            total_batches += 1\n",
    "        print('test perplexity:', np.exp(total_loss / total_batches))\n",
    "\n",
    "    # Bookkeeping        \n",
    "    writer.close()\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
