{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "np.set_printoptions(precision=4, linewidth=200)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.reader import ptb_raw_data\n",
    "from utils.batcher import ptb_batcher\n",
    "from utils.conditional_scope import cond_name_scope, cond_variable_scope\n",
    "from utils.unrolled_rnn import make_rnn_variables\n",
    "from utils.unrolled_rnn import make_rnn_outputs\n",
    "from utils.unrolled_rnn import make_summary_nodes\n",
    "from utils.unrolled_rnn import make_placeholders\n",
    "from utils.unrolled_rnn import make_train_op\n",
    "from utils.batcher import generate_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, vocab_size = ptb_raw_data('bigdata/simple-examples/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE=32\n",
    "HIDDEN_SIZE=128\n",
    "BATCH_SIZE=64\n",
    "NUM_STEPS=8\n",
    "NUM_EPOCHS_INIT_LR=2\n",
    "NUM_EPOCHS_TOTAL=6\n",
    "INITIAL_LR=1e1\n",
    "LR_DECAY_RATE=0.75\n",
    "MAX_NORM=0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "placeholders = make_placeholders(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_steps=NUM_STEPS\n",
    ")\n",
    "rnn_vars = make_rnn_variables(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    ")\n",
    "rnn_outputs = make_rnn_outputs(\n",
    "    input_sequence=placeholders['inputs'],\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_steps=NUM_STEPS,\n",
    "    rnn_variables=rnn_vars\n",
    ")\n",
    "summary_nodes = make_summary_nodes(\n",
    "    targets=placeholders['targets'],\n",
    "    logits=rnn_outputs['logits'],\n",
    ")\n",
    "training_nodes = make_train_op(\n",
    "    summary_nodes['loss'],\n",
    "    placeholders['learning_rate'],\n",
    "    placeholders['max_norm'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 64    loss: 6.734375    gradient norm: 0.2236328125     correct words: 25\n",
      "step: 128    loss: 6.703125    gradient norm: 0.2305908203125     correct words: 33\n",
      "step: 192    loss: 6.453125    gradient norm: 0.2344970703125     correct words: 44\n",
      "step: 256    loss: 6.390625    gradient norm: 0.25537109375     correct words: 42\n",
      "step: 320    loss: 6.20703125    gradient norm: 0.255859375     correct words: 77\n",
      "step: 384    loss: 6.2890625    gradient norm: 0.272216796875     correct words: 49\n",
      "step: 448    loss: 6.1640625    gradient norm: 0.2352294921875     correct words: 75\n",
      "step: 512    loss: 5.8125    gradient norm: 0.265380859375     correct words: 66\n",
      "step: 576    loss: 5.7734375    gradient norm: 0.382568359375     correct words: 72\n",
      "step: 640    loss: 5.6875    gradient norm: 0.265869140625     correct words: 80\n",
      "step: 704    loss: 5.83984375    gradient norm: 0.28955078125     correct words: 84\n",
      "step: 768    loss: 5.90625    gradient norm: 0.375732421875     correct words: 73\n",
      "step: 832    loss: 5.703125    gradient norm: 0.26513671875     correct words: 79\n",
      "step: 896    loss: 5.578125    gradient norm: 0.250244140625     correct words: 101\n",
      "step: 960    loss: 5.453125    gradient norm: 0.29833984375     correct words: 80\n",
      "step: 1024    loss: 5.890625    gradient norm: 0.277587890625     correct words: 77\n",
      "step: 1088    loss: 5.8125    gradient norm: 0.271240234375     correct words: 68\n",
      "step: 1152    loss: 5.546875    gradient norm: 0.26513671875     correct words: 78\n",
      "step: 1216    loss: 5.5390625    gradient norm: 0.260009765625     correct words: 91\n",
      "step: 1280    loss: 5.78125    gradient norm: 0.2822265625     correct words: 81\n",
      "step: 1344    loss: 5.6484375    gradient norm: 0.324462890625     correct words: 83\n",
      "step: 1408    loss: 5.2734375    gradient norm: 0.26171875     correct words: 95\n",
      "step: 1472    loss: 5.515625    gradient norm: 0.2783203125     correct words: 89\n",
      "step: 1536    loss: 5.5859375    gradient norm: 0.273193359375     correct words: 82\n",
      "step: 1600    loss: 5.3359375    gradient norm: 0.268310546875     correct words: 93\n",
      "step: 1664    loss: 5.51171875    gradient norm: 0.310302734375     correct words: 90\n",
      "step: 1728    loss: 5.59375    gradient norm: 0.31201171875     correct words: 73\n",
      "step: 1792    loss: 5.515625    gradient norm: 0.276123046875     correct words: 89\n",
      "validation perplexity: 231.612716671\n",
      "test perplexity: 222.968665916\n",
      "step: 64    loss: 5.3125    gradient norm: 0.26904296875     correct words: 90\n",
      "step: 128    loss: 5.3671875    gradient norm: 0.28369140625     correct words: 94\n",
      "step: 192    loss: 5.40625    gradient norm: 0.295166015625     correct words: 93\n",
      "step: 256    loss: 5.34375    gradient norm: 0.293701171875     correct words: 79\n",
      "step: 320    loss: 5.3125    gradient norm: 0.279541015625     correct words: 100\n",
      "step: 384    loss: 5.3515625    gradient norm: 0.287841796875     correct words: 96\n",
      "step: 448    loss: 5.4609375    gradient norm: 0.27783203125     correct words: 90\n",
      "step: 512    loss: 5.09765625    gradient norm: 0.309326171875     correct words: 101\n",
      "step: 576    loss: 5.1328125    gradient norm: 0.326904296875     correct words: 111\n",
      "step: 640    loss: 5.1484375    gradient norm: 0.302490234375     correct words: 107\n",
      "step: 704    loss: 5.28125    gradient norm: 0.309326171875     correct words: 102\n",
      "step: 768    loss: 5.25    gradient norm: 0.349609375     correct words: 95\n",
      "step: 832    loss: 5.23828125    gradient norm: 0.305419921875     correct words: 101\n",
      "step: 896    loss: 5.09375    gradient norm: 0.288330078125     correct words: 125\n",
      "step: 960    loss: 5.03125    gradient norm: 0.310791015625     correct words: 102\n",
      "step: 1024    loss: 5.3671875    gradient norm: 0.30859375     correct words: 98\n",
      "step: 1088    loss: 5.37890625    gradient norm: 0.303955078125     correct words: 89\n",
      "step: 1152    loss: 5.07421875    gradient norm: 0.29345703125     correct words: 90\n",
      "step: 1216    loss: 5.15625    gradient norm: 0.300537109375     correct words: 98\n",
      "step: 1280    loss: 5.3515625    gradient norm: 0.291259765625     correct words: 92\n",
      "step: 1344    loss: 5.2578125    gradient norm: 0.318603515625     correct words: 103\n",
      "step: 1408    loss: 4.90234375    gradient norm: 0.295166015625     correct words: 110\n",
      "step: 1472    loss: 5.1640625    gradient norm: 0.30029296875     correct words: 94\n",
      "step: 1536    loss: 5.25390625    gradient norm: 0.3046875     correct words: 87\n",
      "step: 1600    loss: 5.015625    gradient norm: 0.295654296875     correct words: 100\n",
      "step: 1664    loss: 5.15625    gradient norm: 0.32763671875     correct words: 95\n",
      "step: 1728    loss: 5.3046875    gradient norm: 0.32861328125     correct words: 87\n",
      "step: 1792    loss: 5.1953125    gradient norm: 0.3115234375     correct words: 97\n",
      "validation perplexity: 189.973754151\n",
      "test perplexity: 181.294371185\n",
      "step: 64    loss: 5.05859375    gradient norm: 0.296875     correct words: 100\n",
      "step: 128    loss: 5.01953125    gradient norm: 0.314453125     correct words: 108\n",
      "step: 192    loss: 5.04296875    gradient norm: 0.313720703125     correct words: 108\n",
      "step: 256    loss: 5.01953125    gradient norm: 0.30810546875     correct words: 97\n",
      "step: 320    loss: 4.98828125    gradient norm: 0.307861328125     correct words: 113\n",
      "step: 384    loss: 5.0078125    gradient norm: 0.313232421875     correct words: 111\n",
      "step: 448    loss: 5.13671875    gradient norm: 0.3056640625     correct words: 111\n",
      "step: 512    loss: 4.828125    gradient norm: 0.332275390625     correct words: 107\n",
      "step: 576    loss: 4.8515625    gradient norm: 0.330078125     correct words: 118\n",
      "step: 640    loss: 4.8671875    gradient norm: 0.3291015625     correct words: 112\n",
      "step: 704    loss: 4.9765625    gradient norm: 0.3291015625     correct words: 112\n",
      "step: 768    loss: 4.9375    gradient norm: 0.34521484375     correct words: 105\n",
      "step: 832    loss: 4.984375    gradient norm: 0.335205078125     correct words: 112\n",
      "step: 896    loss: 4.8359375    gradient norm: 0.32373046875     correct words: 119\n",
      "step: 960    loss: 4.8046875    gradient norm: 0.34033203125     correct words: 110\n",
      "step: 1024    loss: 5.12890625    gradient norm: 0.334716796875     correct words: 113\n",
      "step: 1088    loss: 5.140625    gradient norm: 0.338134765625     correct words: 95\n",
      "step: 1152    loss: 4.8359375    gradient norm: 0.328857421875     correct words: 107\n",
      "step: 1216    loss: 4.9140625    gradient norm: 0.334228515625     correct words: 108\n",
      "step: 1280    loss: 5.109375    gradient norm: 0.3251953125     correct words: 98\n",
      "step: 1344    loss: 5.0    gradient norm: 0.341552734375     correct words: 105\n",
      "step: 1408    loss: 4.6484375    gradient norm: 0.32958984375     correct words: 125\n",
      "step: 1472    loss: 4.92578125    gradient norm: 0.328125     correct words: 112\n",
      "step: 1536    loss: 5.03125    gradient norm: 0.33740234375     correct words: 91\n",
      "step: 1600    loss: 4.78515625    gradient norm: 0.333740234375     correct words: 103\n",
      "step: 1664    loss: 4.8671875    gradient norm: 0.34765625     correct words: 105\n",
      "step: 1728    loss: 5.0703125    gradient norm: 0.3525390625     correct words: 101\n",
      "step: 1792    loss: 4.91796875    gradient norm: 0.342529296875     correct words: 108\n",
      "validation perplexity: 172.958897104\n",
      "test perplexity: 164.330539269\n",
      "step: 64    loss: 4.90625    gradient norm: 0.331298828125     correct words: 105\n",
      "step: 128    loss: 4.828125    gradient norm: 0.347900390625     correct words: 112\n",
      "step: 192    loss: 4.8359375    gradient norm: 0.345458984375     correct words: 108\n",
      "step: 256    loss: 4.828125    gradient norm: 0.34130859375     correct words: 113\n",
      "step: 320    loss: 4.79296875    gradient norm: 0.340576171875     correct words: 121\n",
      "step: 384    loss: 4.8046875    gradient norm: 0.343505859375     correct words: 122\n",
      "step: 448    loss: 4.9375    gradient norm: 0.3349609375     correct words: 124\n",
      "step: 512    loss: 4.6796875    gradient norm: 0.359619140625     correct words: 110\n",
      "step: 576    loss: 4.6875    gradient norm: 0.349853515625     correct words: 116\n",
      "step: 640    loss: 4.6953125    gradient norm: 0.3544921875     correct words: 129\n",
      "step: 704    loss: 4.78125    gradient norm: 0.347900390625     correct words: 114\n",
      "step: 768    loss: 4.75390625    gradient norm: 0.364013671875     correct words: 112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 832    loss: 4.8359375    gradient norm: 0.3642578125     correct words: 110\n",
      "step: 896    loss: 4.6640625    gradient norm: 0.351806640625     correct words: 121\n",
      "step: 960    loss: 4.66015625    gradient norm: 0.36572265625     correct words: 117\n",
      "step: 1024    loss: 4.984375    gradient norm: 0.36083984375     correct words: 115\n",
      "step: 1088    loss: 4.98828125    gradient norm: 0.36767578125     correct words: 104\n",
      "step: 1152    loss: 4.6796875    gradient norm: 0.357421875     correct words: 117\n",
      "step: 1216    loss: 4.7734375    gradient norm: 0.361328125     correct words: 119\n",
      "step: 1280    loss: 4.95703125    gradient norm: 0.35400390625     correct words: 106\n",
      "step: 1344    loss: 4.8125    gradient norm: 0.361328125     correct words: 114\n",
      "step: 1408    loss: 4.4765625    gradient norm: 0.355712890625     correct words: 129\n",
      "step: 1472    loss: 4.73828125    gradient norm: 0.351806640625     correct words: 115\n",
      "step: 1536    loss: 4.8671875    gradient norm: 0.3642578125     correct words: 108\n",
      "step: 1600    loss: 4.6171875    gradient norm: 0.358642578125     correct words: 117\n",
      "step: 1664    loss: 4.6640625    gradient norm: 0.36767578125     correct words: 106\n",
      "step: 1728    loss: 4.890625    gradient norm: 0.376220703125     correct words: 112\n",
      "step: 1792    loss: 4.71875    gradient norm: 0.36767578125     correct words: 118\n",
      "validation perplexity: 166.94746729\n",
      "test perplexity: 158.228166949\n",
      "step: 64    loss: 4.8046875    gradient norm: 0.35986328125     correct words: 120\n",
      "step: 128    loss: 4.71875    gradient norm: 0.373779296875     correct words: 117\n",
      "step: 192    loss: 4.72265625    gradient norm: 0.369873046875     correct words: 121\n",
      "step: 256    loss: 4.703125    gradient norm: 0.364501953125     correct words: 112\n",
      "step: 320    loss: 4.66796875    gradient norm: 0.364990234375     correct words: 126\n",
      "step: 384    loss: 4.68359375    gradient norm: 0.3681640625     correct words: 125\n",
      "step: 448    loss: 4.8125    gradient norm: 0.359619140625     correct words: 133\n",
      "step: 512    loss: 4.578125    gradient norm: 0.381591796875     correct words: 113\n",
      "step: 576    loss: 4.5703125    gradient norm: 0.36962890625     correct words: 126\n",
      "step: 640    loss: 4.58203125    gradient norm: 0.375     correct words: 139\n",
      "step: 704    loss: 4.64453125    gradient norm: 0.36669921875     correct words: 123\n",
      "step: 768    loss: 4.6328125    gradient norm: 0.380859375     correct words: 119\n",
      "step: 832    loss: 4.73828125    gradient norm: 0.3896484375     correct words: 111\n",
      "step: 896    loss: 4.54296875    gradient norm: 0.373291015625     correct words: 124\n",
      "step: 960    loss: 4.546875    gradient norm: 0.38427734375     correct words: 126\n",
      "step: 1024    loss: 4.875    gradient norm: 0.383056640625     correct words: 113\n",
      "step: 1088    loss: 4.859375    gradient norm: 0.3896484375     correct words: 108\n",
      "step: 1152    loss: 4.5546875    gradient norm: 0.377197265625     correct words: 124\n",
      "step: 1216    loss: 4.671875    gradient norm: 0.380615234375     correct words: 122\n",
      "step: 1280    loss: 4.8515625    gradient norm: 0.37646484375     correct words: 109\n",
      "step: 1344    loss: 4.69140625    gradient norm: 0.3779296875     correct words: 121\n",
      "step: 1408    loss: 4.3515625    gradient norm: 0.375732421875     correct words: 133\n",
      "step: 1472    loss: 4.6015625    gradient norm: 0.369140625     correct words: 115\n",
      "step: 1536    loss: 4.7421875    gradient norm: 0.385009765625     correct words: 107\n",
      "step: 1600    loss: 4.4921875    gradient norm: 0.377197265625     correct words: 127\n",
      "step: 1664    loss: 4.53125    gradient norm: 0.3876953125     correct words: 105\n",
      "step: 1728    loss: 4.78125    gradient norm: 0.3955078125     correct words: 117\n",
      "step: 1792    loss: 4.58203125    gradient norm: 0.38671875     correct words: 118\n",
      "validation perplexity: 165.725213296\n",
      "test perplexity: 156.636943244\n",
      "step: 64    loss: 4.7421875    gradient norm: 0.38330078125     correct words: 123\n",
      "step: 128    loss: 4.64453125    gradient norm: 0.395263671875     correct words: 121\n",
      "step: 192    loss: 4.64453125    gradient norm: 0.38916015625     correct words: 126\n",
      "step: 256    loss: 4.609375    gradient norm: 0.3837890625     correct words: 118\n",
      "step: 320    loss: 4.5859375    gradient norm: 0.385009765625     correct words: 129\n",
      "step: 384    loss: 4.6015625    gradient norm: 0.387451171875     correct words: 126\n",
      "step: 448    loss: 4.7265625    gradient norm: 0.379638671875     correct words: 140\n",
      "step: 512    loss: 4.5078125    gradient norm: 0.39794921875     correct words: 121\n",
      "step: 576    loss: 4.49609375    gradient norm: 0.38623046875     correct words: 133\n",
      "step: 640    loss: 4.4921875    gradient norm: 0.3916015625     correct words: 144\n",
      "step: 704    loss: 4.546875    gradient norm: 0.384033203125     correct words: 124\n",
      "step: 768    loss: 4.546875    gradient norm: 0.39599609375     correct words: 120\n",
      "step: 832    loss: 4.6796875    gradient norm: 0.410400390625     correct words: 119\n",
      "step: 896    loss: 4.45703125    gradient norm: 0.391357421875     correct words: 133\n",
      "step: 960    loss: 4.4765625    gradient norm: 0.39892578125     correct words: 129\n",
      "step: 1024    loss: 4.796875    gradient norm: 0.40380859375     correct words: 116\n",
      "step: 1088    loss: 4.7734375    gradient norm: 0.408203125     correct words: 114\n",
      "step: 1152    loss: 4.47265625    gradient norm: 0.394287109375     correct words: 128\n",
      "step: 1216    loss: 4.609375    gradient norm: 0.39599609375     correct words: 126\n",
      "step: 1280    loss: 4.78125    gradient norm: 0.394287109375     correct words: 111\n",
      "step: 1344    loss: 4.6015625    gradient norm: 0.393310546875     correct words: 122\n",
      "step: 1408    loss: 4.26953125    gradient norm: 0.39208984375     correct words: 133\n",
      "step: 1472    loss: 4.5078125    gradient norm: 0.3828125     correct words: 119\n",
      "step: 1536    loss: 4.65625    gradient norm: 0.402099609375     correct words: 110\n",
      "step: 1600    loss: 4.3984375    gradient norm: 0.392578125     correct words: 131\n",
      "step: 1664    loss: 4.453125    gradient norm: 0.40625     correct words: 107\n",
      "step: 1728    loss: 4.703125    gradient norm: 0.413330078125     correct words: 116\n",
      "step: 1792    loss: 4.4921875    gradient norm: 0.401123046875     correct words: 119\n",
      "validation perplexity: 167.07977156\n",
      "test perplexity: 157.468984588\n"
     ]
    }
   ],
   "source": [
    "training_outputs = {**summary_nodes, **training_nodes}\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Bookkeeping\n",
    "    run_id = time.time()\n",
    "    writer = tf.summary.FileWriter('logs/{0}'.format(run_id), sess.graph)\n",
    "    coord = tf.train.Coordinator()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    learning_rate = INITIAL_LR\n",
    "    max_norm = MAX_NORM\n",
    "    for i in range(NUM_EPOCHS_TOTAL):\n",
    "        if i >= NUM_EPOCHS_INIT_LR:\n",
    "            learning_rate *= LR_DECAY_RATE\n",
    "        for batch_idx, (inputs, targets) in enumerate(generate_epoch(X_train, BATCH_SIZE, NUM_STEPS)):\n",
    "            outputs = sess.run(\n",
    "                training_outputs,\n",
    "                feed_dict={\n",
    "                    placeholders['inputs']: inputs,\n",
    "                    placeholders['targets']: targets,\n",
    "                    placeholders['learning_rate']: learning_rate,\n",
    "                    placeholders['max_norm']: max_norm,\n",
    "                }\n",
    "            )\n",
    "            if (batch_idx % 64 == 63):\n",
    "                print('step: {0}    loss: {1}    gradient norm: {2}     correct words: {3}'.format(\n",
    "                    batch_idx+1,\n",
    "                    outputs['loss'],\n",
    "                    outputs['gradient_global_norm'],\n",
    "                    outputs['num_correct_predictions'],\n",
    "                ))\n",
    "                \n",
    "        total_loss, total_batches = 0, 0\n",
    "        for inputs, targets in generate_epoch(X_val, BATCH_SIZE, NUM_STEPS):\n",
    "            outputs = sess.run(\n",
    "                summary_nodes,\n",
    "                feed_dict={\n",
    "                    placeholders['inputs']: inputs,\n",
    "                    placeholders['targets']: targets\n",
    "                },\n",
    "            )\n",
    "            total_loss += outputs['loss']\n",
    "            total_batches += 1\n",
    "        print('validation perplexity:', np.exp(total_loss / total_batches))\n",
    "        total_loss, total_batches = 0, 0\n",
    "        for inputs, targets in generate_epoch(X_test, BATCH_SIZE, NUM_STEPS):\n",
    "            outputs = sess.run(\n",
    "                summary_nodes,\n",
    "                feed_dict={\n",
    "                    placeholders['inputs']: inputs,\n",
    "                    placeholders['targets']: targets\n",
    "                },\n",
    "            )\n",
    "            total_loss += outputs['loss']\n",
    "            total_batches += 1\n",
    "        print('test perplexity:', np.exp(total_loss / total_batches))\n",
    "\n",
    "    # Bookkeeping        \n",
    "    writer.close()\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
