{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "np.set_printoptions(precision=4, linewidth=200)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.reader import ptb_raw_data\n",
    "from utils.batcher import ptb_batcher\n",
    "from utils.conditional_scope import cond_name_scope, cond_variable_scope\n",
    "from utils.unrolled_rnn import make_rnn_variables\n",
    "from utils.unrolled_rnn import make_rnn_outputs\n",
    "from utils.unrolled_rnn import make_summary_nodes\n",
    "from utils.unrolled_rnn import make_placeholders\n",
    "from utils.unrolled_rnn import make_train_op\n",
    "from utils.batcher import generate_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, vocab_size = ptb_raw_data('bigdata/simple-examples/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE=64\n",
    "HIDDEN_SIZE=64\n",
    "BATCH_SIZE=128\n",
    "NUM_STEPS=8\n",
    "NUM_EPOCHS_INIT_LR=4\n",
    "NUM_EPOCHS_TOTAL=12\n",
    "INITIAL_LR=3e-4\n",
    "LR_DECAY_RATE=0.75\n",
    "MAX_NORM=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "placeholders = make_placeholders(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_steps=NUM_STEPS\n",
    ")\n",
    "rnn_vars = make_rnn_variables(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    ")\n",
    "rnn_outputs = make_rnn_outputs(\n",
    "    input_sequence=placeholders['inputs'],\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_steps=NUM_STEPS,\n",
    "    rnn_variables=rnn_vars\n",
    ")\n",
    "summary_nodes = make_summary_nodes(\n",
    "    targets=placeholders['targets'],\n",
    "    logits=rnn_outputs['logits'],\n",
    ")\n",
    "training_nodes = make_train_op(\n",
    "    summary_nodes['loss'],\n",
    "    placeholders['learning_rate'],\n",
    "    placeholders['max_norm'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 64    loss: 9.203125    gradient norm: 0.129638671875     correct words: 0\n",
      "step: 128    loss: 9.1953125    gradient norm: 0.116455078125     correct words: 55\n",
      "step: 192    loss: 9.171875    gradient norm: 0.1461181640625     correct words: 79\n",
      "step: 256    loss: 9.078125    gradient norm: 0.218017578125     correct words: 61\n",
      "step: 320    loss: 8.734375    gradient norm: 0.6064453125     correct words: 71\n",
      "step: 384    loss: 7.8828125    gradient norm: 0.6142578125     correct words: 53\n",
      "step: 448    loss: 7.4375    gradient norm: 0.395263671875     correct words: 51\n",
      "step: 512    loss: 7.046875    gradient norm: 0.280517578125     correct words: 69\n",
      "step: 576    loss: 6.98828125    gradient norm: 0.22802734375     correct words: 58\n",
      "step: 640    loss: 6.95703125    gradient norm: 0.259765625     correct words: 51\n",
      "step: 704    loss: 6.828125    gradient norm: 0.238037109375     correct words: 66\n",
      "step: 768    loss: 7.0078125    gradient norm: 0.254638671875     correct words: 40\n",
      "step: 832    loss: 6.87109375    gradient norm: 0.255615234375     correct words: 45\n",
      "step: 896    loss: 6.8203125    gradient norm: 0.2474365234375     correct words: 63\n",
      "validation perplexity: 900.776385719\n",
      "test perplexity: 861.261726147\n",
      "step: 64    loss: 6.671875    gradient norm: 0.2291259765625     correct words: 63\n",
      "step: 128    loss: 6.890625    gradient norm: 0.2369384765625     correct words: 55\n",
      "step: 192    loss: 6.734375    gradient norm: 0.2447509765625     correct words: 79\n",
      "step: 256    loss: 6.75    gradient norm: 0.216552734375     correct words: 61\n",
      "step: 320    loss: 6.74609375    gradient norm: 0.24462890625     correct words: 71\n",
      "step: 384    loss: 6.7890625    gradient norm: 0.2421875     correct words: 53\n",
      "step: 448    loss: 6.7890625    gradient norm: 0.227783203125     correct words: 51\n",
      "step: 512    loss: 6.5859375    gradient norm: 0.23095703125     correct words: 69\n",
      "step: 576    loss: 6.66015625    gradient norm: 0.2081298828125     correct words: 58\n",
      "step: 640    loss: 6.6640625    gradient norm: 0.250244140625     correct words: 51\n",
      "step: 704    loss: 6.5859375    gradient norm: 0.23583984375     correct words: 66\n",
      "step: 768    loss: 6.8203125    gradient norm: 0.25830078125     correct words: 40\n",
      "step: 832    loss: 6.6953125    gradient norm: 0.2548828125     correct words: 45\n",
      "step: 896    loss: 6.69140625    gradient norm: 0.251953125     correct words: 63\n",
      "validation perplexity: 787.835390345\n",
      "test perplexity: 745.214335008\n",
      "step: 64    loss: 6.54296875    gradient norm: 0.2281494140625     correct words: 63\n",
      "step: 128    loss: 6.78125    gradient norm: 0.2396240234375     correct words: 55\n",
      "step: 192    loss: 6.6328125    gradient norm: 0.254638671875     correct words: 79\n",
      "step: 256    loss: 6.64453125    gradient norm: 0.215576171875     correct words: 61\n",
      "step: 320    loss: 6.65625    gradient norm: 0.2486572265625     correct words: 71\n",
      "step: 384    loss: 6.6875    gradient norm: 0.2445068359375     correct words: 53\n",
      "step: 448    loss: 6.71875    gradient norm: 0.2275390625     correct words: 51\n",
      "step: 512    loss: 6.5    gradient norm: 0.22802734375     correct words: 69\n",
      "step: 576    loss: 6.609375    gradient norm: 0.2100830078125     correct words: 58\n",
      "step: 640    loss: 6.58984375    gradient norm: 0.252197265625     correct words: 51\n",
      "step: 704    loss: 6.51953125    gradient norm: 0.2362060546875     correct words: 66\n",
      "step: 768    loss: 6.7578125    gradient norm: 0.2607421875     correct words: 40\n",
      "step: 832    loss: 6.6328125    gradient norm: 0.255126953125     correct words: 45\n",
      "step: 896    loss: 6.63671875    gradient norm: 0.259521484375     correct words: 63\n",
      "validation perplexity: 749.816376387\n",
      "test perplexity: 703.490721561\n",
      "step: 64    loss: 6.48828125    gradient norm: 0.22900390625     correct words: 63\n",
      "step: 128    loss: 6.734375    gradient norm: 0.2435302734375     correct words: 55\n",
      "step: 192    loss: 6.58203125    gradient norm: 0.265625     correct words: 79\n",
      "step: 256    loss: 6.609375    gradient norm: 0.218017578125     correct words: 61\n",
      "step: 320    loss: 6.609375    gradient norm: 0.251953125     correct words: 71\n",
      "step: 384    loss: 6.6484375    gradient norm: 0.246826171875     correct words: 53\n",
      "step: 448    loss: 6.6875    gradient norm: 0.227294921875     correct words: 51\n",
      "step: 512    loss: 6.4609375    gradient norm: 0.2288818359375     correct words: 69\n",
      "step: 576    loss: 6.59375    gradient norm: 0.21142578125     correct words: 58\n",
      "step: 640    loss: 6.56640625    gradient norm: 0.254150390625     correct words: 51\n",
      "step: 704    loss: 6.50390625    gradient norm: 0.23583984375     correct words: 66\n",
      "step: 768    loss: 6.7421875    gradient norm: 0.261474609375     correct words: 40\n",
      "step: 832    loss: 6.62109375    gradient norm: 0.255126953125     correct words: 45\n",
      "step: 896    loss: 6.625    gradient norm: 0.263671875     correct words: 63\n",
      "validation perplexity: 748.126905422\n",
      "test perplexity: 696.620115621\n",
      "step: 64    loss: 6.47265625    gradient norm: 0.23193359375     correct words: 63\n",
      "step: 128    loss: 6.7265625    gradient norm: 0.2413330078125     correct words: 55\n",
      "step: 192    loss: 6.5625    gradient norm: 0.26513671875     correct words: 79\n",
      "step: 256    loss: 6.609375    gradient norm: 0.2177734375     correct words: 61\n",
      "step: 320    loss: 6.609375    gradient norm: 0.253173828125     correct words: 71\n",
      "step: 384    loss: 6.6484375    gradient norm: 0.246337890625     correct words: 53\n",
      "step: 448    loss: 6.6953125    gradient norm: 0.226318359375     correct words: 51\n",
      "step: 512    loss: 6.4609375    gradient norm: 0.23095703125     correct words: 69\n",
      "step: 576    loss: 6.609375    gradient norm: 0.2109375     correct words: 58\n",
      "step: 640    loss: 6.5625    gradient norm: 0.254150390625     correct words: 51\n",
      "step: 704    loss: 6.5078125    gradient norm: 0.2374267578125     correct words: 66\n",
      "step: 768    loss: 6.7578125    gradient norm: 0.259521484375     correct words: 40\n",
      "step: 832    loss: 6.6171875    gradient norm: 0.255126953125     correct words: 45\n",
      "step: 896    loss: 6.6328125    gradient norm: 0.258544921875     correct words: 63\n",
      "validation perplexity: 760.368220725\n",
      "test perplexity: 701.364233751\n",
      "step: 64    loss: 6.4921875    gradient norm: 0.23486328125     correct words: 63\n",
      "step: 128    loss: 6.7421875    gradient norm: 0.242431640625     correct words: 55\n",
      "step: 192    loss: 6.58203125    gradient norm: 0.2646484375     correct words: 79\n",
      "step: 256    loss: 6.640625    gradient norm: 0.218505859375     correct words: 61\n",
      "step: 320    loss: 6.640625    gradient norm: 0.257080078125     correct words: 71\n",
      "step: 384    loss: 6.67578125    gradient norm: 0.2462158203125     correct words: 53\n",
      "step: 448    loss: 6.7265625    gradient norm: 0.2275390625     correct words: 51\n",
      "step: 512    loss: 6.484375    gradient norm: 0.2325439453125     correct words: 69\n",
      "step: 576    loss: 6.640625    gradient norm: 0.2100830078125     correct words: 58\n",
      "step: 640    loss: 6.58984375    gradient norm: 0.252685546875     correct words: 51\n",
      "step: 704    loss: 6.53125    gradient norm: 0.2371826171875     correct words: 66\n",
      "step: 768    loss: 6.7890625    gradient norm: 0.2607421875     correct words: 40\n",
      "step: 832    loss: 6.640625    gradient norm: 0.253662109375     correct words: 45\n",
      "step: 896    loss: 6.66796875    gradient norm: 0.2548828125     correct words: 63\n",
      "validation perplexity: 785.41181602\n",
      "test perplexity: 719.78681517\n",
      "step: 64    loss: 6.515625    gradient norm: 0.2366943359375     correct words: 63\n",
      "step: 128    loss: 6.76953125    gradient norm: 0.2401123046875     correct words: 55\n",
      "step: 192    loss: 6.59375    gradient norm: 0.26513671875     correct words: 79\n",
      "step: 256    loss: 6.66796875    gradient norm: 0.2193603515625     correct words: 61\n",
      "step: 320    loss: 6.66796875    gradient norm: 0.25830078125     correct words: 71\n",
      "step: 384    loss: 6.7109375    gradient norm: 0.2462158203125     correct words: 53\n",
      "step: 448    loss: 6.7578125    gradient norm: 0.2237548828125     correct words: 51\n",
      "step: 512    loss: 6.5    gradient norm: 0.2314453125     correct words: 69\n",
      "step: 576    loss: 6.67578125    gradient norm: 0.20849609375     correct words: 58\n",
      "step: 640    loss: 6.609375    gradient norm: 0.25048828125     correct words: 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 704    loss: 6.55859375    gradient norm: 0.239013671875     correct words: 66\n",
      "step: 768    loss: 6.8359375    gradient norm: 0.260009765625     correct words: 40\n",
      "step: 832    loss: 6.65625    gradient norm: 0.24853515625     correct words: 45\n",
      "step: 896    loss: 6.6953125    gradient norm: 0.2484130859375     correct words: 63\n",
      "validation perplexity: 816.788936503\n",
      "test perplexity: 742.30901965\n",
      "step: 64    loss: 6.5546875    gradient norm: 0.2406005859375     correct words: 63\n",
      "step: 128    loss: 6.8046875    gradient norm: 0.23974609375     correct words: 55\n",
      "step: 192    loss: 6.6171875    gradient norm: 0.266357421875     correct words: 79\n",
      "step: 256    loss: 6.703125    gradient norm: 0.222412109375     correct words: 61\n",
      "step: 320    loss: 6.703125    gradient norm: 0.2646484375     correct words: 71\n",
      "step: 384    loss: 6.7421875    gradient norm: 0.24951171875     correct words: 53\n",
      "step: 448    loss: 6.80078125    gradient norm: 0.226806640625     correct words: 51\n",
      "step: 512    loss: 6.515625    gradient norm: 0.235595703125     correct words: 69\n",
      "step: 576    loss: 6.7109375    gradient norm: 0.213623046875     correct words: 58\n",
      "step: 640    loss: 6.6328125    gradient norm: 0.251708984375     correct words: 51\n",
      "step: 704    loss: 6.5859375    gradient norm: 0.243408203125     correct words: 66\n",
      "step: 768    loss: 6.875    gradient norm: 0.263916015625     correct words: 40\n",
      "step: 832    loss: 6.671875    gradient norm: 0.2509765625     correct words: 45\n",
      "step: 896    loss: 6.7265625    gradient norm: 0.252685546875     correct words: 63\n",
      "validation perplexity: 840.494052569\n",
      "test perplexity: 759.689772791\n",
      "step: 64    loss: 6.59375    gradient norm: 0.253662109375     correct words: 63\n",
      "step: 128    loss: 6.85546875    gradient norm: 0.24853515625     correct words: 55\n",
      "step: 192    loss: 6.65625    gradient norm: 0.275634765625     correct words: 79\n",
      "step: 256    loss: 6.75390625    gradient norm: 0.231689453125     correct words: 61\n",
      "step: 320    loss: 6.76953125    gradient norm: 0.278076171875     correct words: 71\n",
      "step: 384    loss: 6.80859375    gradient norm: 0.260009765625     correct words: 53\n",
      "step: 448    loss: 6.87109375    gradient norm: 0.234130859375     correct words: 51\n",
      "step: 512    loss: 6.55859375    gradient norm: 0.2454833984375     correct words: 69\n",
      "step: 576    loss: 6.7890625    gradient norm: 0.2232666015625     correct words: 58\n",
      "step: 640    loss: 6.6875    gradient norm: 0.253662109375     correct words: 51\n",
      "step: 704    loss: 6.64453125    gradient norm: 0.25390625     correct words: 66\n",
      "step: 768    loss: 6.953125    gradient norm: 0.264892578125     correct words: 40\n",
      "step: 832    loss: 6.7265625    gradient norm: 0.2509765625     correct words: 45\n",
      "step: 896    loss: 6.80078125    gradient norm: 0.25732421875     correct words: 63\n",
      "validation perplexity: 900.825945642\n",
      "test perplexity: 809.041011124\n",
      "step: 64    loss: 6.6640625    gradient norm: 0.262451171875     correct words: 63\n",
      "step: 128    loss: 6.9375    gradient norm: 0.2490234375     correct words: 55\n",
      "step: 192    loss: 6.71875    gradient norm: 0.27734375     correct words: 79\n",
      "step: 256    loss: 6.8359375    gradient norm: 0.2369384765625     correct words: 61\n",
      "step: 320    loss: 6.84375    gradient norm: 0.287353515625     correct words: 71\n",
      "step: 384    loss: 6.890625    gradient norm: 0.26806640625     correct words: 53\n",
      "step: 448    loss: 6.953125    gradient norm: 0.23974609375     correct words: 51\n",
      "step: 512    loss: 6.60546875    gradient norm: 0.2509765625     correct words: 69\n",
      "step: 576    loss: 6.8515625    gradient norm: 0.22998046875     correct words: 58\n",
      "step: 640    loss: 6.75    gradient norm: 0.263427734375     correct words: 51\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-b53c417b15f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                     \u001b[0mplaceholders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'targets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0mplaceholders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                     \u001b[0mplaceholders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_norm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 }\n\u001b[1;32m     25\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_outputs = {**summary_nodes, **training_nodes}\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Bookkeeping\n",
    "    run_id = time.time()\n",
    "    writer = tf.summary.FileWriter('logs/{0}'.format(run_id), sess.graph)\n",
    "    coord = tf.train.Coordinator()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    learning_rate = INITIAL_LR\n",
    "    max_norm = MAX_NORM\n",
    "    for i in range(NUM_EPOCHS_TOTAL):\n",
    "        if i >= NUM_EPOCHS_INIT_LR:\n",
    "            learning_rate *= LR_DECAY_RATE\n",
    "        for batch_idx, (inputs, targets) in enumerate(generate_epoch(X_train, BATCH_SIZE, NUM_STEPS)):\n",
    "            outputs = sess.run(\n",
    "                training_outputs,\n",
    "                feed_dict={\n",
    "                    placeholders['inputs']: inputs,\n",
    "                    placeholders['targets']: targets,\n",
    "                    placeholders['learning_rate']: learning_rate,\n",
    "                    placeholders['max_norm']: max_norm,\n",
    "                }\n",
    "            )\n",
    "            if (batch_idx % 64 == 63):\n",
    "                print('step: {0}    loss: {1}    gradient norm: {2}     correct words: {3}'.format(\n",
    "                    batch_idx+1,\n",
    "                    outputs['loss'],\n",
    "                    outputs['gradient_global_norm'],\n",
    "                    outputs['num_correct_predictions'],\n",
    "                ))\n",
    "                \n",
    "        total_loss, total_batches = 0, 0\n",
    "        for inputs, targets in generate_epoch(X_val, BATCH_SIZE, NUM_STEPS):\n",
    "            outputs = sess.run(\n",
    "                summary_nodes,\n",
    "                feed_dict={\n",
    "                    placeholders['inputs']: inputs,\n",
    "                    placeholders['targets']: targets\n",
    "                },\n",
    "            )\n",
    "            total_loss += outputs['loss']\n",
    "            total_batches += 1\n",
    "        print('validation perplexity:', np.exp(total_loss / total_batches))\n",
    "        total_loss, total_batches = 0, 0\n",
    "        for inputs, targets in generate_epoch(X_test, BATCH_SIZE, NUM_STEPS):\n",
    "            outputs = sess.run(\n",
    "                summary_nodes,\n",
    "                feed_dict={\n",
    "                    placeholders['inputs']: inputs,\n",
    "                    placeholders['targets']: targets\n",
    "                },\n",
    "            )\n",
    "            total_loss += outputs['loss']\n",
    "            total_batches += 1\n",
    "        print('test perplexity:', np.exp(total_loss / total_batches))\n",
    "\n",
    "    # Bookkeeping        \n",
    "    writer.close()\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
