{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "np.set_printoptions(precision=4, linewidth=200)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.reader import ptb_raw_data\n",
    "from utils.batcher import ptb_batcher\n",
    "from utils.conditional_scope import cond_name_scope, cond_variable_scope\n",
    "from utils.unrolled_rnn import make_rnn_variables\n",
    "from utils.unrolled_rnn import make_rnn_outputs\n",
    "from utils.unrolled_rnn import make_summary_nodes\n",
    "from utils.unrolled_rnn import make_placeholders\n",
    "from utils.unrolled_rnn import make_train_op\n",
    "from utils.batcher import generate_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, vocab_size = ptb_raw_data('bigdata/simple-examples/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE=64\n",
    "HIDDEN_SIZE=256\n",
    "BATCH_SIZE=32\n",
    "NUM_STEPS=16\n",
    "NUM_EPOCHS_INIT_LR=3\n",
    "NUM_EPOCHS_TOTAL=8\n",
    "INITIAL_LR=5e0\n",
    "LR_DECAY_RATE=0.75\n",
    "MAX_NORM=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "placeholders = make_placeholders(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_steps=NUM_STEPS\n",
    ")\n",
    "rnn_vars = make_rnn_variables(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    ")\n",
    "rnn_outputs = make_rnn_outputs(\n",
    "    input_sequence=placeholders['inputs'],\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_steps=NUM_STEPS,\n",
    "    rnn_variables=rnn_vars\n",
    ")\n",
    "summary_nodes = make_summary_nodes(\n",
    "    targets=placeholders['targets'],\n",
    "    logits=rnn_outputs['logits'],\n",
    ")\n",
    "training_nodes = make_train_op(\n",
    "    summary_nodes['loss'],\n",
    "    placeholders['learning_rate'],\n",
    "    placeholders['max_norm'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 64    loss: 7.71875    gradient norm: 2.517578125     correct words: 22\n",
      "step: 128    loss: 9.6171875    gradient norm: 5.125     correct words: 29\n",
      "step: 192    loss: 6.8828125    gradient norm: 0.325439453125     correct words: 45\n",
      "step: 256    loss: 6.7265625    gradient norm: 0.53271484375     correct words: 36\n",
      "step: 320    loss: 6.60546875    gradient norm: 0.4287109375     correct words: 28\n",
      "step: 384    loss: 6.640625    gradient norm: 0.49853515625     correct words: 43\n",
      "step: 448    loss: 6.71484375    gradient norm: 0.6025390625     correct words: 22\n",
      "step: 512    loss: 6.640625    gradient norm: 0.484130859375     correct words: 33\n",
      "step: 576    loss: 6.9921875    gradient norm: 1.7861328125     correct words: 72\n",
      "step: 640    loss: 6.4296875    gradient norm: 0.650390625     correct words: 28\n",
      "step: 704    loss: 6.2109375    gradient norm: 0.48046875     correct words: 46\n",
      "step: 768    loss: 6.4375    gradient norm: 0.5185546875     correct words: 45\n",
      "step: 832    loss: 6.39453125    gradient norm: 0.560546875     correct words: 42\n",
      "step: 896    loss: 6.2734375    gradient norm: 1.453125     correct words: 67\n",
      "step: 960    loss: 6.234375    gradient norm: 0.5576171875     correct words: 68\n",
      "step: 1024    loss: 6.67578125    gradient norm: 1.396484375     correct words: 50\n",
      "step: 1088    loss: 6.37109375    gradient norm: 1.283203125     correct words: 48\n",
      "step: 1152    loss: 6.5    gradient norm: 1.4296875     correct words: 58\n",
      "step: 1216    loss: 6.10546875    gradient norm: 0.6669921875     correct words: 49\n",
      "step: 1280    loss: 5.8203125    gradient norm: 0.43212890625     correct words: 75\n",
      "step: 1344    loss: 5.8515625    gradient norm: 0.60205078125     correct words: 65\n",
      "step: 1408    loss: 5.8828125    gradient norm: 0.50048828125     correct words: 72\n",
      "step: 1472    loss: 6.34375    gradient norm: 0.5595703125     correct words: 45\n",
      "step: 1536    loss: 6.15625    gradient norm: 0.47802734375     correct words: 48\n",
      "step: 1600    loss: 6.04296875    gradient norm: 1.0634765625     correct words: 78\n",
      "step: 1664    loss: 6.00390625    gradient norm: 0.5732421875     correct words: 56\n",
      "step: 1728    loss: 6.5078125    gradient norm: 1.7958984375     correct words: 78\n",
      "step: 1792    loss: 5.78125    gradient norm: 0.70654296875     correct words: 71\n",
      "validation perplexity: 384.797837494\n",
      "test perplexity: 361.728364081\n",
      "step: 64    loss: 6.03515625    gradient norm: 1.115234375     correct words: 69\n",
      "step: 128    loss: 6.0234375    gradient norm: 0.53076171875     correct words: 52\n",
      "step: 192    loss: 6.2734375    gradient norm: 1.3349609375     correct words: 59\n",
      "step: 256    loss: 5.94140625    gradient norm: 0.71484375     correct words: 72\n",
      "step: 320    loss: 5.5625    gradient norm: 0.70166015625     correct words: 94\n",
      "step: 384    loss: 5.9296875    gradient norm: 1.16796875     correct words: 83\n",
      "step: 448    loss: 5.84375    gradient norm: 0.44970703125     correct words: 72\n",
      "step: 512    loss: 5.9140625    gradient norm: 0.446044921875     correct words: 67\n",
      "step: 576    loss: 5.7890625    gradient norm: 0.93896484375     correct words: 66\n",
      "step: 640    loss: 5.640625    gradient norm: 0.55224609375     correct words: 77\n",
      "step: 704    loss: 5.8671875    gradient norm: 0.68359375     correct words: 65\n",
      "step: 768    loss: 5.7734375    gradient norm: 0.55419921875     correct words: 68\n",
      "step: 832    loss: 5.63671875    gradient norm: 0.476806640625     correct words: 82\n",
      "step: 896    loss: 5.5859375    gradient norm: 0.640625     correct words: 70\n",
      "step: 960    loss: 5.49609375    gradient norm: 0.8759765625     correct words: 83\n",
      "step: 1024    loss: 5.875    gradient norm: 0.5439453125     correct words: 77\n",
      "step: 1088    loss: 6.0859375    gradient norm: 1.8779296875     correct words: 74\n",
      "step: 1152    loss: 5.8671875    gradient norm: 0.63623046875     correct words: 83\n",
      "step: 1216    loss: 5.52734375    gradient norm: 0.53271484375     correct words: 87\n",
      "step: 1280    loss: 5.6015625    gradient norm: 1.15234375     correct words: 71\n",
      "step: 1344    loss: 5.546875    gradient norm: 0.55224609375     correct words: 83\n",
      "step: 1408    loss: 5.5703125    gradient norm: 0.5751953125     correct words: 79\n",
      "step: 1472    loss: 5.52734375    gradient norm: 0.61962890625     correct words: 93\n",
      "step: 1536    loss: 6.0859375    gradient norm: 1.462890625     correct words: 72\n",
      "step: 1600    loss: 5.6328125    gradient norm: 0.492919921875     correct words: 77\n",
      "step: 1664    loss: 5.5    gradient norm: 0.43798828125     correct words: 94\n",
      "step: 1728    loss: 5.7265625    gradient norm: 0.52587890625     correct words: 76\n",
      "step: 1792    loss: 5.640625    gradient norm: 0.578125     correct words: 66\n",
      "validation perplexity: 316.663478106\n",
      "test perplexity: 308.550265618\n",
      "step: 64    loss: 5.69140625    gradient norm: 0.56689453125     correct words: 81\n",
      "step: 128    loss: 5.671875    gradient norm: 0.95166015625     correct words: 79\n",
      "step: 192    loss: 5.671875    gradient norm: 0.52978515625     correct words: 76\n",
      "step: 256    loss: 5.640625    gradient norm: 1.0703125     correct words: 72\n",
      "step: 320    loss: 5.34375    gradient norm: 0.462646484375     correct words: 96\n",
      "step: 384    loss: 5.703125    gradient norm: 1.271484375     correct words: 85\n",
      "step: 448    loss: 5.6015625    gradient norm: 0.4697265625     correct words: 84\n",
      "step: 512    loss: 5.6875    gradient norm: 0.515625     correct words: 74\n",
      "step: 576    loss: 5.453125    gradient norm: 0.6630859375     correct words: 81\n",
      "step: 640    loss: 5.38671875    gradient norm: 0.6025390625     correct words: 83\n",
      "step: 704    loss: 5.3359375    gradient norm: 0.61767578125     correct words: 89\n",
      "step: 768    loss: 5.46875    gradient norm: 0.57080078125     correct words: 81\n",
      "step: 832    loss: 5.2890625    gradient norm: 0.73486328125     correct words: 91\n",
      "step: 896    loss: 5.078125    gradient norm: 0.6416015625     correct words: 101\n",
      "step: 960    loss: 5.2265625    gradient norm: 0.60009765625     correct words: 83\n",
      "step: 1024    loss: 5.7109375    gradient norm: 0.94384765625     correct words: 99\n",
      "step: 1088    loss: 5.59375    gradient norm: 1.2744140625     correct words: 100\n",
      "step: 1152    loss: 5.57421875    gradient norm: 0.5107421875     correct words: 87\n",
      "step: 1216    loss: 5.3359375    gradient norm: 0.58837890625     correct words: 94\n",
      "step: 1280    loss: 5.2578125    gradient norm: 0.5732421875     correct words: 100\n",
      "step: 1344    loss: 5.3359375    gradient norm: 0.951171875     correct words: 90\n",
      "step: 1408    loss: 5.3359375    gradient norm: 0.6015625     correct words: 77\n",
      "step: 1472    loss: 5.203125    gradient norm: 0.5     correct words: 104\n",
      "step: 1536    loss: 5.4921875    gradient norm: 0.94580078125     correct words: 90\n",
      "step: 1600    loss: 5.3046875    gradient norm: 0.84228515625     correct words: 106\n",
      "step: 1664    loss: 5.4296875    gradient norm: 1.1708984375     correct words: 106\n",
      "step: 1728    loss: 5.55078125    gradient norm: 1.3212890625     correct words: 74\n",
      "step: 1792    loss: 5.21875    gradient norm: 0.65380859375     correct words: 82\n",
      "validation perplexity: 248.067032439\n",
      "test perplexity: 240.63382199\n",
      "step: 64    loss: 5.21875    gradient norm: 0.544921875     correct words: 98\n",
      "step: 128    loss: 5.5546875    gradient norm: 1.173828125     correct words: 90\n",
      "step: 192    loss: 5.109375    gradient norm: 0.52001953125     correct words: 89\n",
      "step: 256    loss: 5.203125    gradient norm: 0.56640625     correct words: 92\n",
      "step: 320    loss: 4.9375    gradient norm: 0.6806640625     correct words: 112\n",
      "step: 384    loss: 5.109375    gradient norm: 0.5634765625     correct words: 101\n",
      "step: 448    loss: 5.26953125    gradient norm: 0.51416015625     correct words: 84\n",
      "step: 512    loss: 5.33203125    gradient norm: 0.55615234375     correct words: 90\n",
      "step: 576    loss: 4.96875    gradient norm: 0.56396484375     correct words: 111\n",
      "step: 640    loss: 4.8359375    gradient norm: 0.5908203125     correct words: 105\n",
      "step: 704    loss: 5.06640625    gradient norm: 0.607421875     correct words: 94\n",
      "step: 768    loss: 5.01953125    gradient norm: 0.60791015625     correct words: 90\n",
      "step: 832    loss: 4.90625    gradient norm: 0.541015625     correct words: 95\n",
      "step: 896    loss: 4.80078125    gradient norm: 0.521484375     correct words: 110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 960    loss: 4.88671875    gradient norm: 0.58984375     correct words: 115\n",
      "step: 1024    loss: 5.65625    gradient norm: 1.634765625     correct words: 97\n",
      "step: 1088    loss: 5.08203125    gradient norm: 0.853515625     correct words: 111\n",
      "step: 1152    loss: 5.1640625    gradient norm: 0.5244140625     correct words: 111\n",
      "step: 1216    loss: 4.734375    gradient norm: 0.50537109375     correct words: 114\n",
      "step: 1280    loss: 4.8046875    gradient norm: 0.578125     correct words: 114\n",
      "step: 1344    loss: 4.953125    gradient norm: 0.56591796875     correct words: 101\n",
      "step: 1408    loss: 4.796875    gradient norm: 0.53955078125     correct words: 96\n",
      "step: 1472    loss: 5.140625    gradient norm: 0.615234375     correct words: 108\n",
      "step: 1536    loss: 5.1484375    gradient norm: 0.8564453125     correct words: 103\n",
      "step: 1600    loss: 4.9921875    gradient norm: 0.5185546875     correct words: 115\n",
      "step: 1664    loss: 4.953125    gradient norm: 0.51806640625     correct words: 109\n",
      "step: 1728    loss: 5.1875    gradient norm: 0.56640625     correct words: 102\n",
      "step: 1792    loss: 4.984375    gradient norm: 0.57958984375     correct words: 95\n",
      "validation perplexity: 181.775489683\n",
      "test perplexity: 173.983411931\n",
      "step: 64    loss: 5.01953125    gradient norm: 0.61181640625     correct words: 106\n",
      "step: 128    loss: 5.109375    gradient norm: 0.479248046875     correct words: 103\n",
      "step: 192    loss: 4.8828125    gradient norm: 0.499267578125     correct words: 106\n",
      "step: 256    loss: 4.8984375    gradient norm: 0.5458984375     correct words: 105\n",
      "step: 320    loss: 4.74609375    gradient norm: 0.6552734375     correct words: 120\n",
      "step: 384    loss: 4.875    gradient norm: 0.47998046875     correct words: 109\n",
      "step: 448    loss: 5.0390625    gradient norm: 0.4794921875     correct words: 95\n",
      "step: 512    loss: 4.953125    gradient norm: 0.5361328125     correct words: 102\n",
      "step: 576    loss: 4.734375    gradient norm: 0.53271484375     correct words: 121\n",
      "step: 640    loss: 4.62890625    gradient norm: 0.55322265625     correct words: 115\n",
      "step: 704    loss: 4.6875    gradient norm: 0.51513671875     correct words: 112\n",
      "step: 768    loss: 4.84375    gradient norm: 0.59619140625     correct words: 100\n",
      "step: 832    loss: 4.66015625    gradient norm: 0.5087890625     correct words: 114\n",
      "step: 896    loss: 4.640625    gradient norm: 0.564453125     correct words: 119\n",
      "step: 960    loss: 4.671875    gradient norm: 0.56591796875     correct words: 123\n",
      "step: 1024    loss: 5.078125    gradient norm: 0.56982421875     correct words: 102\n",
      "step: 1088    loss: 4.91015625    gradient norm: 0.59716796875     correct words: 120\n",
      "step: 1152    loss: 4.87109375    gradient norm: 0.52880859375     correct words: 116\n",
      "step: 1216    loss: 4.57421875    gradient norm: 0.56201171875     correct words: 131\n",
      "step: 1280    loss: 4.609375    gradient norm: 0.57861328125     correct words: 122\n",
      "step: 1344    loss: 4.671875    gradient norm: 0.56591796875     correct words: 99\n",
      "step: 1408    loss: 4.5703125    gradient norm: 0.55859375     correct words: 110\n",
      "step: 1472    loss: 4.7734375    gradient norm: 0.56884765625     correct words: 123\n",
      "step: 1536    loss: 4.9296875    gradient norm: 0.67529296875     correct words: 110\n",
      "step: 1600    loss: 4.74609375    gradient norm: 0.541015625     correct words: 122\n",
      "step: 1664    loss: 4.6796875    gradient norm: 0.52294921875     correct words: 129\n",
      "step: 1728    loss: 4.84375    gradient norm: 0.560546875     correct words: 117\n",
      "step: 1792    loss: 4.765625    gradient norm: 0.6044921875     correct words: 111\n",
      "validation perplexity: 161.227744146\n",
      "test perplexity: 153.584811089\n",
      "step: 64    loss: 4.828125    gradient norm: 0.52587890625     correct words: 109\n",
      "step: 128    loss: 5.0    gradient norm: 0.65478515625     correct words: 104\n",
      "step: 192    loss: 4.70703125    gradient norm: 0.51513671875     correct words: 112\n",
      "step: 256    loss: 4.69140625    gradient norm: 0.53662109375     correct words: 118\n",
      "step: 320    loss: 4.5859375    gradient norm: 0.57080078125     correct words: 128\n",
      "step: 384    loss: 4.7421875    gradient norm: 0.5146484375     correct words: 111\n",
      "step: 448    loss: 4.94140625    gradient norm: 0.55615234375     correct words: 106\n",
      "step: 512    loss: 4.8046875    gradient norm: 0.52490234375     correct words: 110\n",
      "step: 576    loss: 4.609375    gradient norm: 0.54443359375     correct words: 128\n",
      "step: 640    loss: 4.4765625    gradient norm: 0.55029296875     correct words: 122\n",
      "step: 704    loss: 4.5546875    gradient norm: 0.5419921875     correct words: 122\n",
      "step: 768    loss: 4.71875    gradient norm: 0.56982421875     correct words: 108\n",
      "step: 832    loss: 4.4765625    gradient norm: 0.5302734375     correct words: 127\n",
      "step: 896    loss: 4.515625    gradient norm: 0.54296875     correct words: 130\n",
      "step: 960    loss: 4.5    gradient norm: 0.5859375     correct words: 130\n",
      "step: 1024    loss: 4.94921875    gradient norm: 0.76904296875     correct words: 111\n",
      "step: 1088    loss: 4.7109375    gradient norm: 0.58740234375     correct words: 130\n",
      "step: 1152    loss: 4.7421875    gradient norm: 0.60302734375     correct words: 128\n",
      "step: 1216    loss: 4.4609375    gradient norm: 0.58935546875     correct words: 138\n",
      "step: 1280    loss: 4.4609375    gradient norm: 0.57177734375     correct words: 131\n",
      "step: 1344    loss: 4.5390625    gradient norm: 0.595703125     correct words: 106\n",
      "step: 1408    loss: 4.3984375    gradient norm: 0.5751953125     correct words: 117\n",
      "step: 1472    loss: 4.62890625    gradient norm: 0.5888671875     correct words: 130\n",
      "step: 1536    loss: 4.765625    gradient norm: 0.59423828125     correct words: 108\n",
      "step: 1600    loss: 4.58203125    gradient norm: 0.5869140625     correct words: 133\n",
      "step: 1664    loss: 4.53515625    gradient norm: 0.53369140625     correct words: 132\n",
      "step: 1728    loss: 4.6953125    gradient norm: 0.55517578125     correct words: 120\n",
      "step: 1792    loss: 4.58203125    gradient norm: 0.56640625     correct words: 120\n",
      "validation perplexity: 152.543874597\n",
      "test perplexity: 144.19154339\n",
      "step: 64    loss: 4.71875    gradient norm: 0.5439453125     correct words: 116\n",
      "step: 128    loss: 4.8984375    gradient norm: 0.63037109375     correct words: 113\n",
      "step: 192    loss: 4.6015625    gradient norm: 0.55615234375     correct words: 117\n",
      "step: 256    loss: 4.578125    gradient norm: 0.5537109375     correct words: 125\n",
      "step: 320    loss: 4.48046875    gradient norm: 0.5576171875     correct words: 127\n",
      "step: 384    loss: 4.6484375    gradient norm: 0.53564453125     correct words: 112\n",
      "step: 448    loss: 4.8828125    gradient norm: 0.62451171875     correct words: 108\n",
      "step: 512    loss: 4.70703125    gradient norm: 0.5537109375     correct words: 119\n",
      "step: 576    loss: 4.5234375    gradient norm: 0.56884765625     correct words: 129\n",
      "step: 640    loss: 4.38671875    gradient norm: 0.56787109375     correct words: 126\n",
      "step: 704    loss: 4.4609375    gradient norm: 0.56640625     correct words: 123\n",
      "step: 768    loss: 4.640625    gradient norm: 0.58203125     correct words: 107\n",
      "step: 832    loss: 4.359375    gradient norm: 0.55078125     correct words: 140\n",
      "step: 896    loss: 4.4375    gradient norm: 0.5654296875     correct words: 138\n",
      "step: 960    loss: 4.37890625    gradient norm: 0.59130859375     correct words: 130\n",
      "step: 1024    loss: 4.83984375    gradient norm: 0.69287109375     correct words: 117\n",
      "step: 1088    loss: 4.609375    gradient norm: 0.5849609375     correct words: 139\n",
      "step: 1152    loss: 4.64453125    gradient norm: 0.58447265625     correct words: 128\n",
      "step: 1216    loss: 4.390625    gradient norm: 0.6103515625     correct words: 143\n",
      "step: 1280    loss: 4.3828125    gradient norm: 0.6005859375     correct words: 135\n",
      "step: 1344    loss: 4.4453125    gradient norm: 0.6240234375     correct words: 108\n",
      "step: 1408    loss: 4.28125    gradient norm: 0.5869140625     correct words: 126\n",
      "step: 1472    loss: 4.5234375    gradient norm: 0.6064453125     correct words: 132\n",
      "step: 1536    loss: 4.64453125    gradient norm: 0.5986328125     correct words: 112\n",
      "step: 1600    loss: 4.46875    gradient norm: 0.58935546875     correct words: 134\n",
      "step: 1664    loss: 4.4375    gradient norm: 0.568359375     correct words: 140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1728    loss: 4.59375    gradient norm: 0.58447265625     correct words: 120\n",
      "step: 1792    loss: 4.48828125    gradient norm: 0.5771484375     correct words: 119\n",
      "validation perplexity: 149.759878754\n",
      "test perplexity: 140.844472546\n",
      "step: 64    loss: 4.6484375    gradient norm: 0.56396484375     correct words: 119\n",
      "step: 128    loss: 4.8359375    gradient norm: 0.62744140625     correct words: 115\n",
      "step: 192    loss: 4.53515625    gradient norm: 0.59375     correct words: 124\n",
      "step: 256    loss: 4.51171875    gradient norm: 0.583984375     correct words: 130\n",
      "step: 320    loss: 4.4140625    gradient norm: 0.57666015625     correct words: 131\n",
      "step: 384    loss: 4.59375    gradient norm: 0.55419921875     correct words: 114\n",
      "step: 448    loss: 4.82421875    gradient norm: 0.6357421875     correct words: 111\n",
      "step: 512    loss: 4.640625    gradient norm: 0.57470703125     correct words: 126\n",
      "step: 576    loss: 4.46875    gradient norm: 0.5927734375     correct words: 132\n",
      "step: 640    loss: 4.33203125    gradient norm: 0.587890625     correct words: 134\n",
      "step: 704    loss: 4.40625    gradient norm: 0.58642578125     correct words: 120\n",
      "step: 768    loss: 4.578125    gradient norm: 0.5986328125     correct words: 112\n",
      "step: 832    loss: 4.28125    gradient norm: 0.5693359375     correct words: 147\n",
      "step: 896    loss: 4.390625    gradient norm: 0.5810546875     correct words: 143\n",
      "step: 960    loss: 4.296875    gradient norm: 0.59765625     correct words: 137\n",
      "step: 1024    loss: 4.7734375    gradient norm: 0.6982421875     correct words: 120\n",
      "step: 1088    loss: 4.546875    gradient norm: 0.59130859375     correct words: 145\n",
      "step: 1152    loss: 4.5859375    gradient norm: 0.58251953125     correct words: 133\n",
      "step: 1216    loss: 4.328125    gradient norm: 0.61572265625     correct words: 147\n",
      "step: 1280    loss: 4.3359375    gradient norm: 0.62841796875     correct words: 132\n",
      "step: 1344    loss: 4.3828125    gradient norm: 0.64599609375     correct words: 115\n",
      "step: 1408    loss: 4.203125    gradient norm: 0.59716796875     correct words: 130\n",
      "step: 1472    loss: 4.453125    gradient norm: 0.61279296875     correct words: 133\n",
      "step: 1536    loss: 4.5703125    gradient norm: 0.6162109375     correct words: 117\n",
      "step: 1600    loss: 4.3984375    gradient norm: 0.6025390625     correct words: 139\n",
      "step: 1664    loss: 4.3828125    gradient norm: 0.61376953125     correct words: 142\n",
      "step: 1728    loss: 4.5234375    gradient norm: 0.60888671875     correct words: 122\n",
      "step: 1792    loss: 4.4296875    gradient norm: 0.59423828125     correct words: 124\n",
      "validation perplexity: 149.83302155\n",
      "test perplexity: 140.185809029\n"
     ]
    }
   ],
   "source": [
    "training_outputs = {**summary_nodes, **training_nodes}\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Bookkeeping\n",
    "    run_id = time.time()\n",
    "    writer = tf.summary.FileWriter('logs/{0}'.format(run_id), sess.graph)\n",
    "    coord = tf.train.Coordinator()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    learning_rate = INITIAL_LR\n",
    "    max_norm = MAX_NORM\n",
    "    for i in range(NUM_EPOCHS_TOTAL):\n",
    "        if i >= NUM_EPOCHS_INIT_LR:\n",
    "            learning_rate *= LR_DECAY_RATE\n",
    "        for batch_idx, (inputs, targets) in enumerate(generate_epoch(X_train, BATCH_SIZE, NUM_STEPS)):\n",
    "            outputs = sess.run(\n",
    "                training_outputs,\n",
    "                feed_dict={\n",
    "                    placeholders['inputs']: inputs,\n",
    "                    placeholders['targets']: targets,\n",
    "                    placeholders['learning_rate']: learning_rate,\n",
    "                    placeholders['max_norm']: max_norm,\n",
    "                }\n",
    "            )\n",
    "            if (batch_idx % 64 == 63):\n",
    "                print('step: {0}    loss: {1}    gradient norm: {2}     correct words: {3}'.format(\n",
    "                    batch_idx+1,\n",
    "                    outputs['loss'],\n",
    "                    outputs['gradient_global_norm'],\n",
    "                    outputs['num_correct_predictions'],\n",
    "                ))\n",
    "                \n",
    "        total_loss, total_batches = 0, 0\n",
    "        for inputs, targets in generate_epoch(X_val, BATCH_SIZE, NUM_STEPS):\n",
    "            outputs = sess.run(\n",
    "                summary_nodes,\n",
    "                feed_dict={\n",
    "                    placeholders['inputs']: inputs,\n",
    "                    placeholders['targets']: targets\n",
    "                },\n",
    "            )\n",
    "            total_loss += outputs['loss']\n",
    "            total_batches += 1\n",
    "        print('validation perplexity:', np.exp(total_loss / total_batches))\n",
    "        total_loss, total_batches = 0, 0\n",
    "        for inputs, targets in generate_epoch(X_test, BATCH_SIZE, NUM_STEPS):\n",
    "            outputs = sess.run(\n",
    "                summary_nodes,\n",
    "                feed_dict={\n",
    "                    placeholders['inputs']: inputs,\n",
    "                    placeholders['targets']: targets\n",
    "                },\n",
    "            )\n",
    "            total_loss += outputs['loss']\n",
    "            total_batches += 1\n",
    "        print('test perplexity:', np.exp(total_loss / total_batches))\n",
    "\n",
    "    # Bookkeeping        \n",
    "    writer.close()\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
