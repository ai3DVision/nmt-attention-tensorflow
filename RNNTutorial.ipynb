{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "np.set_printoptions(precision=4, linewidth=200)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.reader import ptb_raw_data\n",
    "from utils.batcher import ptb_batcher\n",
    "from utils.conditional_scope import cond_name_scope, cond_variable_scope\n",
    "from utils.unrolled_rnn import make_rnn_variables\n",
    "from utils.unrolled_rnn import make_rnn_outputs\n",
    "from utils.unrolled_rnn import make_summary_nodes\n",
    "from utils.unrolled_rnn import make_placeholders\n",
    "from utils.unrolled_rnn import make_train_op\n",
    "from utils.batcher import generate_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, vocab_size = ptb_raw_data('bigdata/simple-examples/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE=64\n",
    "HIDDEN_SIZE=256\n",
    "BATCH_SIZE=32\n",
    "NUM_STEPS=16\n",
    "NUM_EPOCHS_INIT_LR=3\n",
    "NUM_EPOCHS_TOTAL=8\n",
    "INITIAL_LR=5e0\n",
    "LR_DECAY_RATE=0.75\n",
    "MAX_NORM=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "placeholders = make_placeholders(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_steps=NUM_STEPS\n",
    ")\n",
    "rnn_vars = make_rnn_variables(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    initializer_scale=0.2\n",
    ")\n",
    "rnn_outputs = make_rnn_outputs(\n",
    "    input_sequence=placeholders['inputs'],\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_steps=NUM_STEPS,\n",
    "    rnn_variables=rnn_vars\n",
    ")\n",
    "summary_nodes = make_summary_nodes(\n",
    "    targets=placeholders['targets'],\n",
    "    logits=rnn_outputs['logits'],\n",
    ")\n",
    "training_nodes = make_train_op(\n",
    "    summary_nodes['loss'],\n",
    "    placeholders['learning_rate'],\n",
    "    placeholders['max_norm'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 64    loss: 7.5703125    gradient norm: 1.0771484375     correct words: 25\n",
      "step: 128    loss: 7.84375    gradient norm: 4.05078125     correct words: 37\n",
      "step: 192    loss: 6.8359375    gradient norm: 0.57861328125     correct words: 41\n",
      "step: 256    loss: 6.734375    gradient norm: 1.404296875     correct words: 49\n",
      "step: 320    loss: 6.421875    gradient norm: 1.1083984375     correct words: 69\n",
      "step: 384    loss: 6.453125    gradient norm: 0.57080078125     correct words: 55\n",
      "step: 448    loss: 6.98046875    gradient norm: 1.943359375     correct words: 50\n",
      "step: 512    loss: 6.765625    gradient norm: 1.4970703125     correct words: 51\n",
      "step: 576    loss: 6.62890625    gradient norm: 1.7841796875     correct words: 59\n",
      "step: 640    loss: 6.1875    gradient norm: 1.212890625     correct words: 67\n",
      "step: 704    loss: 6.125    gradient norm: 0.60693359375     correct words: 61\n",
      "step: 768    loss: 6.19140625    gradient norm: 0.62451171875     correct words: 48\n",
      "step: 832    loss: 6.140625    gradient norm: 0.85107421875     correct words: 70\n",
      "step: 896    loss: 6.02734375    gradient norm: 0.6552734375     correct words: 55\n",
      "step: 960    loss: 5.83984375    gradient norm: 0.6142578125     correct words: 70\n",
      "step: 1024    loss: 6.3125    gradient norm: 0.5244140625     correct words: 75\n",
      "step: 1088    loss: 6.140625    gradient norm: 1.7509765625     correct words: 73\n",
      "step: 1152    loss: 6.125    gradient norm: 0.51318359375     correct words: 73\n",
      "step: 1216    loss: 5.76953125    gradient norm: 0.61767578125     correct words: 67\n",
      "step: 1280    loss: 5.88671875    gradient norm: 0.69580078125     correct words: 55\n",
      "step: 1344    loss: 5.4921875    gradient norm: 0.564453125     correct words: 77\n",
      "step: 1408    loss: 5.82421875    gradient norm: 0.525390625     correct words: 78\n",
      "step: 1472    loss: 6.03125    gradient norm: 0.5830078125     correct words: 60\n",
      "step: 1536    loss: 6.0546875    gradient norm: 0.6279296875     correct words: 61\n",
      "step: 1600    loss: 5.66796875    gradient norm: 0.765625     correct words: 92\n",
      "step: 1664    loss: 5.73828125    gradient norm: 0.5185546875     correct words: 67\n",
      "step: 1728    loss: 5.734375    gradient norm: 0.544921875     correct words: 82\n",
      "step: 1792    loss: 5.7734375    gradient norm: 0.59130859375     correct words: 70\n",
      "validation perplexity: 356.488536385\n",
      "test perplexity: 347.743397315\n",
      "step: 64    loss: 5.859375    gradient norm: 1.02734375     correct words: 75\n",
      "step: 128    loss: 6.4609375    gradient norm: 1.896484375     correct words: 71\n",
      "step: 192    loss: 5.75390625    gradient norm: 0.537109375     correct words: 72\n",
      "step: 256    loss: 5.609375    gradient norm: 0.625     correct words: 82\n",
      "step: 320    loss: 5.515625    gradient norm: 0.62255859375     correct words: 82\n",
      "step: 384    loss: 5.4921875    gradient norm: 0.7333984375     correct words: 83\n",
      "step: 448    loss: 5.7578125    gradient norm: 0.5048828125     correct words: 73\n",
      "step: 512    loss: 5.8125    gradient norm: 0.99853515625     correct words: 76\n",
      "step: 576    loss: 5.66015625    gradient norm: 0.97412109375     correct words: 89\n",
      "step: 640    loss: 5.546875    gradient norm: 0.63818359375     correct words: 85\n",
      "step: 704    loss: 5.3984375    gradient norm: 1.0341796875     correct words: 79\n",
      "step: 768    loss: 5.546875    gradient norm: 0.638671875     correct words: 83\n",
      "step: 832    loss: 5.4765625    gradient norm: 0.50927734375     correct words: 94\n",
      "step: 896    loss: 5.31640625    gradient norm: 0.91552734375     correct words: 96\n",
      "step: 960    loss: 5.1953125    gradient norm: 0.91845703125     correct words: 91\n",
      "step: 1024    loss: 5.8984375    gradient norm: 1.2958984375     correct words: 93\n",
      "step: 1088    loss: 5.953125    gradient norm: 2.033203125     correct words: 76\n",
      "step: 1152    loss: 5.5859375    gradient norm: 1.00390625     correct words: 92\n",
      "step: 1216    loss: 5.578125    gradient norm: 0.78857421875     correct words: 86\n",
      "step: 1280    loss: 5.17578125    gradient norm: 0.82177734375     correct words: 74\n",
      "step: 1344    loss: 5.3671875    gradient norm: 0.58056640625     correct words: 91\n",
      "step: 1408    loss: 5.34375    gradient norm: 0.54052734375     correct words: 98\n",
      "step: 1472    loss: 5.53125    gradient norm: 0.5634765625     correct words: 98\n",
      "step: 1536    loss: 5.46484375    gradient norm: 0.8544921875     correct words: 79\n",
      "step: 1600    loss: 5.5234375    gradient norm: 1.021484375     correct words: 106\n",
      "step: 1664    loss: 5.390625    gradient norm: 0.521484375     correct words: 89\n",
      "step: 1728    loss: 5.4296875    gradient norm: 1.03515625     correct words: 107\n",
      "step: 1792    loss: 5.4140625    gradient norm: 0.56298828125     correct words: 85\n",
      "validation perplexity: 270.511515931\n",
      "test perplexity: 263.511412882\n",
      "step: 64    loss: 5.37890625    gradient norm: 0.65673828125     correct words: 75\n",
      "step: 128    loss: 5.5078125    gradient norm: 0.5380859375     correct words: 89\n",
      "step: 192    loss: 5.3359375    gradient norm: 0.55859375     correct words: 92\n",
      "step: 256    loss: 5.46484375    gradient norm: 1.0693359375     correct words: 86\n",
      "step: 320    loss: 5.046875    gradient norm: 0.75244140625     correct words: 93\n",
      "step: 384    loss: 5.1875    gradient norm: 0.67529296875     correct words: 94\n",
      "step: 448    loss: 5.44921875    gradient norm: 0.5029296875     correct words: 90\n",
      "step: 512    loss: 5.4921875    gradient norm: 0.96435546875     correct words: 82\n",
      "step: 576    loss: 5.25390625    gradient norm: 0.59765625     correct words: 91\n",
      "step: 640    loss: 5.28125    gradient norm: 0.7138671875     correct words: 83\n",
      "step: 704    loss: 5.1484375    gradient norm: 0.5732421875     correct words: 82\n",
      "step: 768    loss: 5.2421875    gradient norm: 1.01953125     correct words: 99\n",
      "step: 832    loss: 5.0390625    gradient norm: 0.66455078125     correct words: 101\n",
      "step: 896    loss: 5.26171875    gradient norm: 0.6279296875     correct words: 93\n",
      "step: 960    loss: 5.13671875    gradient norm: 0.60693359375     correct words: 84\n",
      "step: 1024    loss: 5.60546875    gradient norm: 1.1689453125     correct words: 101\n",
      "step: 1088    loss: 5.3671875    gradient norm: 0.6298828125     correct words: 102\n",
      "step: 1152    loss: 5.4140625    gradient norm: 0.60498046875     correct words: 94\n",
      "step: 1216    loss: 5.1640625    gradient norm: 0.62841796875     correct words: 99\n",
      "step: 1280    loss: 5.0234375    gradient norm: 0.6474609375     correct words: 100\n",
      "step: 1344    loss: 5.03125    gradient norm: 0.5859375     correct words: 97\n",
      "step: 1408    loss: 5.09375    gradient norm: 0.59521484375     correct words: 96\n",
      "step: 1472    loss: 5.28125    gradient norm: 0.84765625     correct words: 92\n",
      "step: 1536    loss: 5.296875    gradient norm: 0.63330078125     correct words: 88\n",
      "step: 1600    loss: 5.4765625    gradient norm: 1.30859375     correct words: 105\n",
      "step: 1664    loss: 5.203125    gradient norm: 0.5810546875     correct words: 100\n",
      "step: 1728    loss: 5.3125    gradient norm: 0.53515625     correct words: 108\n",
      "step: 1792    loss: 5.2421875    gradient norm: 0.97998046875     correct words: 93\n",
      "validation perplexity: 202.449935024\n",
      "test perplexity: 190.231582933\n",
      "step: 64    loss: 5.09375    gradient norm: 0.68359375     correct words: 92\n",
      "step: 128    loss: 5.18359375    gradient norm: 0.5234375     correct words: 92\n",
      "step: 192    loss: 5.015625    gradient norm: 0.56884765625     correct words: 100\n",
      "step: 256    loss: 5.06640625    gradient norm: 0.52197265625     correct words: 101\n",
      "step: 320    loss: 4.8046875    gradient norm: 0.609375     correct words: 116\n",
      "step: 384    loss: 4.9453125    gradient norm: 0.517578125     correct words: 119\n",
      "step: 448    loss: 5.140625    gradient norm: 0.50439453125     correct words: 97\n",
      "step: 512    loss: 5.09375    gradient norm: 0.483154296875     correct words: 97\n",
      "step: 576    loss: 4.84375    gradient norm: 0.5693359375     correct words: 110\n",
      "step: 640    loss: 4.7890625    gradient norm: 0.61376953125     correct words: 110\n",
      "step: 704    loss: 4.9296875    gradient norm: 0.7841796875     correct words: 91\n",
      "step: 768    loss: 4.890625    gradient norm: 0.5283203125     correct words: 110\n",
      "step: 832    loss: 4.80859375    gradient norm: 0.80419921875     correct words: 111\n",
      "step: 896    loss: 4.78125    gradient norm: 0.66064453125     correct words: 112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 960    loss: 4.7734375    gradient norm: 0.62109375     correct words: 110\n",
      "step: 1024    loss: 5.2109375    gradient norm: 0.7880859375     correct words: 109\n",
      "step: 1088    loss: 5.0    gradient norm: 0.58837890625     correct words: 105\n",
      "step: 1152    loss: 5.0078125    gradient norm: 0.564453125     correct words: 102\n",
      "step: 1216    loss: 4.7109375    gradient norm: 0.61572265625     correct words: 117\n",
      "step: 1280    loss: 4.72265625    gradient norm: 0.62060546875     correct words: 122\n",
      "step: 1344    loss: 4.75    gradient norm: 0.58544921875     correct words: 103\n",
      "step: 1408    loss: 4.671875    gradient norm: 0.70654296875     correct words: 121\n",
      "step: 1472    loss: 4.96875    gradient norm: 0.556640625     correct words: 115\n",
      "step: 1536    loss: 4.984375    gradient norm: 0.93994140625     correct words: 102\n",
      "step: 1600    loss: 4.9375    gradient norm: 0.59521484375     correct words: 111\n",
      "step: 1664    loss: 4.8671875    gradient norm: 0.5595703125     correct words: 113\n",
      "step: 1728    loss: 5.0625    gradient norm: 1.1533203125     correct words: 99\n",
      "step: 1792    loss: 4.890625    gradient norm: 0.60009765625     correct words: 108\n",
      "validation perplexity: 167.236987344\n",
      "test perplexity: 159.586007332\n",
      "step: 64    loss: 4.890625    gradient norm: 0.56298828125     correct words: 102\n",
      "step: 128    loss: 4.9765625    gradient norm: 0.5419921875     correct words: 104\n",
      "step: 192    loss: 4.7734375    gradient norm: 0.74072265625     correct words: 109\n",
      "step: 256    loss: 4.84375    gradient norm: 0.6005859375     correct words: 104\n",
      "step: 320    loss: 4.6875    gradient norm: 0.720703125     correct words: 123\n",
      "step: 384    loss: 4.6953125    gradient norm: 0.50927734375     correct words: 124\n",
      "step: 448    loss: 4.96875    gradient norm: 0.52197265625     correct words: 106\n",
      "step: 512    loss: 4.9453125    gradient norm: 0.52978515625     correct words: 96\n",
      "step: 576    loss: 4.6484375    gradient norm: 0.52587890625     correct words: 121\n",
      "step: 640    loss: 4.6015625    gradient norm: 0.61376953125     correct words: 121\n",
      "step: 704    loss: 4.703125    gradient norm: 0.61474609375     correct words: 111\n",
      "step: 768    loss: 4.7421875    gradient norm: 0.61083984375     correct words: 114\n",
      "step: 832    loss: 4.5546875    gradient norm: 0.53564453125     correct words: 123\n",
      "step: 896    loss: 4.640625    gradient norm: 0.58203125     correct words: 123\n",
      "step: 960    loss: 4.56640625    gradient norm: 0.59228515625     correct words: 121\n",
      "step: 1024    loss: 5.13671875    gradient norm: 1.0263671875     correct words: 113\n",
      "step: 1088    loss: 4.71484375    gradient norm: 0.5595703125     correct words: 117\n",
      "step: 1152    loss: 4.78125    gradient norm: 0.54541015625     correct words: 116\n",
      "step: 1216    loss: 4.5    gradient norm: 0.6044921875     correct words: 123\n",
      "step: 1280    loss: 4.53515625    gradient norm: 0.61669921875     correct words: 126\n",
      "step: 1344    loss: 4.5546875    gradient norm: 0.58837890625     correct words: 108\n",
      "step: 1408    loss: 4.5546875    gradient norm: 0.58203125     correct words: 120\n",
      "step: 1472    loss: 4.75390625    gradient norm: 0.58349609375     correct words: 124\n",
      "step: 1536    loss: 4.734375    gradient norm: 0.548828125     correct words: 117\n",
      "step: 1600    loss: 4.734375    gradient norm: 0.54638671875     correct words: 126\n",
      "step: 1664    loss: 4.625    gradient norm: 0.56884765625     correct words: 121\n",
      "step: 1728    loss: 4.80078125    gradient norm: 0.5732421875     correct words: 117\n",
      "step: 1792    loss: 4.75390625    gradient norm: 0.623046875     correct words: 105\n",
      "validation perplexity: 163.51368733\n",
      "test perplexity: 154.562815409\n",
      "step: 64    loss: 4.7578125    gradient norm: 0.5458984375     correct words: 107\n",
      "step: 128    loss: 4.91796875    gradient norm: 0.7392578125     correct words: 108\n",
      "step: 192    loss: 4.58984375    gradient norm: 0.53759765625     correct words: 126\n",
      "step: 256    loss: 4.6640625    gradient norm: 0.56982421875     correct words: 112\n",
      "step: 320    loss: 4.5859375    gradient norm: 0.6240234375     correct words: 126\n",
      "step: 384    loss: 4.5859375    gradient norm: 0.51220703125     correct words: 129\n",
      "step: 448    loss: 4.859375    gradient norm: 0.5703125     correct words: 105\n",
      "step: 512    loss: 4.8359375    gradient norm: 0.55615234375     correct words: 108\n",
      "step: 576    loss: 4.5546875    gradient norm: 0.54150390625     correct words: 131\n",
      "step: 640    loss: 4.48046875    gradient norm: 0.5712890625     correct words: 126\n",
      "step: 704    loss: 4.546875    gradient norm: 0.59619140625     correct words: 125\n",
      "step: 768    loss: 4.62890625    gradient norm: 0.56396484375     correct words: 119\n",
      "step: 832    loss: 4.3984375    gradient norm: 0.5283203125     correct words: 133\n",
      "step: 896    loss: 4.53125    gradient norm: 0.54736328125     correct words: 126\n",
      "step: 960    loss: 4.42578125    gradient norm: 0.60498046875     correct words: 125\n",
      "step: 1024    loss: 4.99609375    gradient norm: 0.82568359375     correct words: 119\n",
      "step: 1088    loss: 4.6171875    gradient norm: 0.61181640625     correct words: 125\n",
      "step: 1152    loss: 4.6875    gradient norm: 0.6474609375     correct words: 123\n",
      "step: 1216    loss: 4.39453125    gradient norm: 0.61376953125     correct words: 135\n",
      "step: 1280    loss: 4.375    gradient norm: 0.59619140625     correct words: 138\n",
      "step: 1344    loss: 4.41015625    gradient norm: 0.5849609375     correct words: 117\n",
      "step: 1408    loss: 4.37109375    gradient norm: 0.58203125     correct words: 124\n",
      "step: 1472    loss: 4.640625    gradient norm: 0.58349609375     correct words: 124\n",
      "step: 1536    loss: 4.6171875    gradient norm: 0.5625     correct words: 122\n",
      "step: 1600    loss: 4.625    gradient norm: 0.6220703125     correct words: 129\n",
      "step: 1664    loss: 4.50390625    gradient norm: 0.57080078125     correct words: 133\n",
      "step: 1728    loss: 4.6875    gradient norm: 0.57666015625     correct words: 119\n",
      "step: 1792    loss: 4.59765625    gradient norm: 0.60546875     correct words: 108\n",
      "validation perplexity: 158.311028363\n",
      "test perplexity: 148.714203569\n",
      "step: 64    loss: 4.671875    gradient norm: 0.56005859375     correct words: 114\n",
      "step: 128    loss: 4.8359375    gradient norm: 0.6455078125     correct words: 113\n",
      "step: 192    loss: 4.51953125    gradient norm: 0.56005859375     correct words: 127\n",
      "step: 256    loss: 4.55859375    gradient norm: 0.5615234375     correct words: 121\n",
      "step: 320    loss: 4.51171875    gradient norm: 0.5859375     correct words: 124\n",
      "step: 384    loss: 4.515625    gradient norm: 0.5283203125     correct words: 134\n",
      "step: 448    loss: 4.8203125    gradient norm: 0.61474609375     correct words: 107\n",
      "step: 512    loss: 4.7578125    gradient norm: 0.57763671875     correct words: 111\n",
      "step: 576    loss: 4.5    gradient norm: 0.56298828125     correct words: 135\n",
      "step: 640    loss: 4.40625    gradient norm: 0.57666015625     correct words: 126\n",
      "step: 704    loss: 4.4609375    gradient norm: 0.583984375     correct words: 129\n",
      "step: 768    loss: 4.5625    gradient norm: 0.5810546875     correct words: 124\n",
      "step: 832    loss: 4.31640625    gradient norm: 0.546875     correct words: 135\n",
      "step: 896    loss: 4.453125    gradient norm: 0.55517578125     correct words: 131\n",
      "step: 960    loss: 4.3125    gradient norm: 0.59765625     correct words: 140\n",
      "step: 1024    loss: 4.8984375    gradient norm: 0.6640625     correct words: 123\n",
      "step: 1088    loss: 4.51171875    gradient norm: 0.56787109375     correct words: 136\n",
      "step: 1152    loss: 4.609375    gradient norm: 0.607421875     correct words: 126\n",
      "step: 1216    loss: 4.30859375    gradient norm: 0.607421875     correct words: 137\n",
      "step: 1280    loss: 4.3046875    gradient norm: 0.59521484375     correct words: 140\n",
      "step: 1344    loss: 4.3203125    gradient norm: 0.6015625     correct words: 121\n",
      "step: 1408    loss: 4.26171875    gradient norm: 0.5849609375     correct words: 134\n",
      "step: 1472    loss: 4.5703125    gradient norm: 0.60009765625     correct words: 132\n",
      "step: 1536    loss: 4.546875    gradient norm: 0.57861328125     correct words: 125\n",
      "step: 1600    loss: 4.5390625    gradient norm: 0.6025390625     correct words: 134\n",
      "step: 1664    loss: 4.421875    gradient norm: 0.59326171875     correct words: 144\n",
      "step: 1728    loss: 4.6171875    gradient norm: 0.611328125     correct words: 124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1792    loss: 4.5078125    gradient norm: 0.60693359375     correct words: 114\n",
      "validation perplexity: 157.937850741\n",
      "test perplexity: 147.697463435\n",
      "step: 64    loss: 4.625    gradient norm: 0.57421875     correct words: 124\n",
      "step: 128    loss: 4.7890625    gradient norm: 0.62890625     correct words: 119\n",
      "step: 192    loss: 4.4765625    gradient norm: 0.5830078125     correct words: 130\n",
      "step: 256    loss: 4.4921875    gradient norm: 0.57763671875     correct words: 131\n",
      "step: 320    loss: 4.47265625    gradient norm: 0.5908203125     correct words: 133\n",
      "step: 384    loss: 4.4765625    gradient norm: 0.54248046875     correct words: 144\n",
      "step: 448    loss: 4.7890625    gradient norm: 0.63330078125     correct words: 110\n",
      "step: 512    loss: 4.7109375    gradient norm: 0.5888671875     correct words: 109\n",
      "step: 576    loss: 4.4765625    gradient norm: 0.5869140625     correct words: 129\n",
      "step: 640    loss: 4.3515625    gradient norm: 0.5908203125     correct words: 128\n",
      "step: 704    loss: 4.41796875    gradient norm: 0.59521484375     correct words: 126\n",
      "step: 768    loss: 4.5234375    gradient norm: 0.60009765625     correct words: 123\n",
      "step: 832    loss: 4.2734375    gradient norm: 0.56298828125     correct words: 144\n",
      "step: 896    loss: 4.390625    gradient norm: 0.5634765625     correct words: 138\n",
      "step: 960    loss: 4.2421875    gradient norm: 0.59033203125     correct words: 147\n",
      "step: 1024    loss: 4.83984375    gradient norm: 0.63427734375     correct words: 123\n",
      "step: 1088    loss: 4.453125    gradient norm: 0.57470703125     correct words: 142\n",
      "step: 1152    loss: 4.5546875    gradient norm: 0.5947265625     correct words: 128\n",
      "step: 1216    loss: 4.2578125    gradient norm: 0.5986328125     correct words: 146\n",
      "step: 1280    loss: 4.2734375    gradient norm: 0.6171875     correct words: 143\n",
      "step: 1344    loss: 4.265625    gradient norm: 0.619140625     correct words: 135\n",
      "step: 1408    loss: 4.1875    gradient norm: 0.58154296875     correct words: 140\n",
      "step: 1472    loss: 4.51953125    gradient norm: 0.609375     correct words: 139\n",
      "step: 1536    loss: 4.5078125    gradient norm: 0.591796875     correct words: 125\n",
      "step: 1600    loss: 4.4921875    gradient norm: 0.6044921875     correct words: 138\n",
      "step: 1664    loss: 4.3828125    gradient norm: 0.61767578125     correct words: 148\n",
      "step: 1728    loss: 4.578125    gradient norm: 0.638671875     correct words: 123\n",
      "step: 1792    loss: 4.44921875    gradient norm: 0.61474609375     correct words: 123\n",
      "validation perplexity: 159.557005349\n",
      "test perplexity: 148.297256602\n"
     ]
    }
   ],
   "source": [
    "training_outputs = {**summary_nodes, **training_nodes}\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Bookkeeping\n",
    "    run_id = time.time()\n",
    "    writer = tf.summary.FileWriter('logs/{0}'.format(run_id), sess.graph)\n",
    "    coord = tf.train.Coordinator()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    learning_rate = INITIAL_LR\n",
    "    max_norm = MAX_NORM\n",
    "    for i in range(NUM_EPOCHS_TOTAL):\n",
    "        if i >= NUM_EPOCHS_INIT_LR:\n",
    "            learning_rate *= LR_DECAY_RATE\n",
    "        for batch_idx, (inputs, targets) in enumerate(generate_epoch(X_train, BATCH_SIZE, NUM_STEPS)):\n",
    "            outputs = sess.run(\n",
    "                training_outputs,\n",
    "                feed_dict={\n",
    "                    placeholders['inputs']: inputs,\n",
    "                    placeholders['targets']: targets,\n",
    "                    placeholders['learning_rate']: learning_rate,\n",
    "                    placeholders['max_norm']: max_norm,\n",
    "                }\n",
    "            )\n",
    "            if (batch_idx % 64 == 63):\n",
    "                print('step: {0}    loss: {1}    gradient norm: {2}     correct words: {3}'.format(\n",
    "                    batch_idx+1,\n",
    "                    outputs['loss'],\n",
    "                    outputs['gradient_global_norm'],\n",
    "                    outputs['num_correct_predictions'],\n",
    "                ))\n",
    "                \n",
    "        total_loss, total_batches = 0, 0\n",
    "        for inputs, targets in generate_epoch(X_val, BATCH_SIZE, NUM_STEPS):\n",
    "            outputs = sess.run(\n",
    "                summary_nodes,\n",
    "                feed_dict={\n",
    "                    placeholders['inputs']: inputs,\n",
    "                    placeholders['targets']: targets\n",
    "                },\n",
    "            )\n",
    "            total_loss += outputs['loss']\n",
    "            total_batches += 1\n",
    "        print('validation perplexity:', np.exp(total_loss / total_batches))\n",
    "        total_loss, total_batches = 0, 0\n",
    "        for inputs, targets in generate_epoch(X_test, BATCH_SIZE, NUM_STEPS):\n",
    "            outputs = sess.run(\n",
    "                summary_nodes,\n",
    "                feed_dict={\n",
    "                    placeholders['inputs']: inputs,\n",
    "                    placeholders['targets']: targets\n",
    "                },\n",
    "            )\n",
    "            total_loss += outputs['loss']\n",
    "            total_batches += 1\n",
    "        print('test perplexity:', np.exp(total_loss / total_batches))\n",
    "\n",
    "    # Bookkeeping        \n",
    "    writer.close()\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
