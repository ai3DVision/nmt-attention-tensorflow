{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.reader import ptb_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, vocab = ptb_raw_data('bigdata/simple-examples/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shoutout to: \n",
    "# https://stackoverflow.com/questions/41695893/tensorflow-conditionally-add-variable-scope\n",
    "class empty_scope():\n",
    " def __init__(self):\n",
    "     pass\n",
    " def __enter__(self):\n",
    "     pass\n",
    " def __exit__(self, type, value, traceback):\n",
    "     pass\n",
    "\n",
    "def cond_name_scope(scope):\n",
    "    return empty_scope() if scope is None else tf.name_scope(scope)\n",
    "\n",
    "def cond_variable_scope(scope):\n",
    "    return empty_scope() if scope is None else tf.variable_scope(scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ptb_batcher(raw_data, batch_size, num_steps):\n",
    "    \"\"\"Return a batch of data.\n",
    "\n",
    "    Equivalent of ptb_producer that I wrote to understand all the TF concepts.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('batcher'):\n",
    "        tf_raw_data = tf.convert_to_tensor(\n",
    "            raw_data,\n",
    "            name='raw_data',\n",
    "            dtype=tf.int32\n",
    "        )\n",
    "        data_len = tf.size(\n",
    "            tf_raw_data,\n",
    "            name='num_elems'\n",
    "        )\n",
    "        num_batches = tf.floordiv(\n",
    "            data_len, batch_size,\n",
    "            name='num_batches'\n",
    "        )\n",
    "        data = tf.reshape(\n",
    "            tf_raw_data[:batch_size * num_batches],\n",
    "            [batch_size, num_batches],\n",
    "            name='data'\n",
    "        )\n",
    "        batches_per_epoch = tf.floordiv(\n",
    "            num_batches - 1, num_steps,\n",
    "            name='batches_per_epoch'\n",
    "        )\n",
    "        tf_queue = tf.train.range_input_producer(\n",
    "            limit=batches_per_epoch, shuffle=False\n",
    "        )\n",
    "        i = tf_queue.dequeue(name='iter_idx')\n",
    "        x = tf.identity(\n",
    "            data[:, (i * num_steps):((i+1) * num_steps)], \n",
    "            name='x'\n",
    "        )\n",
    "        x.set_shape([batch_size, num_steps])\n",
    "        y = tf.identity(\n",
    "            data[:, (1 + i * num_steps):(1 + (i+1) * num_steps)],\n",
    "            name='y'\n",
    "        )\n",
    "        y.set_shape([batch_size, num_steps])\n",
    "        return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gru_update(x_t, h_t_minus_1, gru_params, timestep=None):\n",
    "    U_z, W_z, b_z = gru_params['U_z'], gru_params['W_z'], gru_params['b_z']\n",
    "    U_r, W_r, b_r = gru_params['U_r'], gru_params['W_r'], gru_params['b_r']\n",
    "    U_h, W_h, b_h = gru_params['U_h'], gru_params['W_h'], gru_params['b_h']\n",
    "    with tf.name_scope('gru_calculations'):\n",
    "        r_t = tf.sigmoid(\n",
    "            tf.matmul(x_t, W_r) + tf.matmul(h_t_minus_1, U_r) + b_r,\n",
    "            name='r' + (\n",
    "                '_{0}'.format(timestep) if timestep is not None else ''\n",
    "            ),\n",
    "        )\n",
    "        z_t = tf.sigmoid(\n",
    "            tf.matmul(x_t, W_z) + tf.matmul(h_t_minus_1, U_z) + b_z,\n",
    "            name='z' + (\n",
    "                '_{0}'.format(timestep) if timestep is not None else ''\n",
    "            ),\n",
    "        )\n",
    "        h_tilde_t = tf.tanh(\n",
    "            tf.matmul(x_t, W_h) + tf.matmul(h_t_minus_1 * r_t, U_h) + b_h,\n",
    "            name='h_tilde' + (\n",
    "                '_{0}'.format(timestep) if timestep is not None else ''\n",
    "            ),\n",
    "        )\n",
    "        h_t = z_t * h_t_minus_1 + (1 - z_t) * h_tilde_t\n",
    "    return h_t    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_rnn(vocab_size, embedding_size, hidden_size, batch_size, num_steps):\n",
    "    \"\"\"Simplified version of PTBModel\"\"\"\n",
    "    with tf.variable_scope(\n",
    "        'RNNParams',\n",
    "        reuse=False,\n",
    "        initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "    ):\n",
    "        embedding_matrix = tf.get_variable(\n",
    "            'embedding',\n",
    "            [vocab_size, embedding_size],\n",
    "            dtype=tf.float16,\n",
    "        )\n",
    "        gru_params = {\n",
    "            'U_z': tf.get_variable(\n",
    "                'U_z',\n",
    "                [hidden_size, hidden_size],\n",
    "                dtype=tf.float16,\n",
    "            ),\n",
    "            'W_z': tf.get_variable(\n",
    "                'W_z',\n",
    "                [embedding_size, hidden_size],\n",
    "                dtype=tf.float16,\n",
    "            ),\n",
    "            'b_z': tf.get_variable(\n",
    "                'b_z',\n",
    "                [hidden_size],\n",
    "                dtype=tf.float16,\n",
    "            ),\n",
    "            'U_r': tf.get_variable(\n",
    "                'U_r',\n",
    "                [hidden_size, hidden_size],\n",
    "                dtype=tf.float16,\n",
    "            ),\n",
    "            'W_r': tf.get_variable(\n",
    "                'W_r',\n",
    "                [embedding_size, hidden_size],\n",
    "                dtype=tf.float16,\n",
    "            ),\n",
    "            'b_r': tf.get_variable(\n",
    "                'b_r',\n",
    "                [hidden_size],\n",
    "                dtype=tf.float16,\n",
    "            ),\n",
    "            'U_h': tf.get_variable(\n",
    "                'U_h',\n",
    "                [hidden_size, hidden_size],\n",
    "                dtype=tf.float16,\n",
    "            ),\n",
    "            'W_h': tf.get_variable(\n",
    "                'W_h',\n",
    "                [embedding_size, hidden_size],\n",
    "                dtype=tf.float16,\n",
    "            ),\n",
    "            'b_h': tf.get_variable(\n",
    "                'b_h',\n",
    "                [hidden_size],\n",
    "                dtype=tf.float16,\n",
    "            ),\n",
    "        }\n",
    "        softmax_params = {\n",
    "            'W': tf.get_variable(\n",
    "                'softmax_w',\n",
    "                [hidden_size, vocab_size],\n",
    "                dtype=tf.float16,\n",
    "            ),\n",
    "            'b': tf.get_variable(\n",
    "                'softmax_b',\n",
    "                [vocab_size],\n",
    "                dtype=tf.float16,\n",
    "            )\n",
    "        }\n",
    "\n",
    "    with tf.name_scope('RNN'):\n",
    "        input_sequence = tf.placeholder(\n",
    "            tf.int32,\n",
    "            shape=[batch_size, num_steps],\n",
    "            name='input_sequence',\n",
    "        )\n",
    "        embedded_inputs = tf.nn.embedding_lookup(\n",
    "            embedding_matrix,\n",
    "            input_sequence,\n",
    "            name='embedded_inputs',\n",
    "        )\n",
    "        h_start = tf.zeros(\n",
    "            [batch_size, hidden_size],\n",
    "            name='h_start',\n",
    "            dtype=tf.float16,\n",
    "        )\n",
    "        h_prev = h_start\n",
    "        h_states = []\n",
    "        for i in range(num_steps):\n",
    "            h_states.append(gru_update(\n",
    "                embedded_inputs[:, i, :],\n",
    "                h_prev,\n",
    "                gru_params,\n",
    "                i\n",
    "            ))\n",
    "            h_prev = h_states[-1]\n",
    "            \n",
    "        # h_states is a list of tensors, each of which has shape\n",
    "        # (batch_size, hidden_size)\n",
    "        #\n",
    "        # we ultimately want to end up with something of shape \n",
    "        # (batch_size, num_steps, vocab_size)\n",
    "        # \n",
    "        # To see why the steps below work, try the following.\n",
    "        # (In this example, batch_size is 3, hidden_size = 4, num_steps = 2)\n",
    "        # \n",
    "        # m1 = tf.constant(np.reshape(np.arange(12),(3,4)))\n",
    "        # m2 = tf.constant(6 + np.reshape(np.arange(12),(3,4)))\n",
    "        # concatenated_ms = tf.concat([m1, m2], axis=1)\n",
    "        # skinny_ms = tf.reshape(concatenated_ms, [-1, 4])\n",
    "        # reshaped_ms = tf.reshape(skinny_ms, [-1, 2, 4])\n",
    "        # with tf.Session() as sess:\n",
    "        #     for m in sess.run([\n",
    "        #         concatenated_ms,\n",
    "        #         skinny_ms,\n",
    "        #         reshaped_ms\n",
    "        #     ]):\n",
    "        #         print(m)\n",
    "        #         print()\n",
    "        #\n",
    "        # which prints\n",
    "        #\n",
    "        # [[ 0  1  2  3  6  7  8  9]\n",
    "        #  [ 4  5  6  7 10 11 12 13]\n",
    "        #  [ 8  9 10 11 14 15 16 17]]\n",
    "        #\n",
    "        # [[ 0  1  2  3]\n",
    "        #  [ 6  7  8  9]\n",
    "        #  [ 4  5  6  7]\n",
    "        #  [10 11 12 13]\n",
    "        #  [ 8  9 10 11]\n",
    "        #  [14 15 16 17]]\n",
    "        #\n",
    "        # [[[ 0  1  2  3]\n",
    "        #   [ 6  7  8  9]]\n",
    "        #\n",
    "        #  [[ 4  5  6  7]\n",
    "        #   [10 11 12 13]]\n",
    "        #\n",
    "        #  [[ 8  9 10 11]\n",
    "        #   [14 15 16 17]]]\n",
    "        #\n",
    "        # concatenated_states will have shape\n",
    "        # (batch_size, num_steps * hidden_size)\n",
    "        concatenated_states = tf.concat(\n",
    "            h_states,\n",
    "            axis=1,\n",
    "            name='concatenated_states'\n",
    "        )\n",
    "        # long_and_skinny_logits will have shape\n",
    "        # (batch_size * num_steps, hidden_size)\n",
    "        long_and_skinny_logits = tf.nn.xw_plus_b(\n",
    "            concatenated_states,\n",
    "            softmax_params['W'],\n",
    "            softmax_params['B'],\n",
    "            name='long_and_skinny_logits',\n",
    "        )\n",
    "        # logits will have shape \n",
    "        # (batch_size, num_steps, hidden_size)\n",
    "        logits = tf.reshape(\n",
    "            long_and_skinny_logits,\n",
    "            [batch_size, num_steps, hidden_size],\n",
    "            name='logits'\n",
    "        )\n",
    "        \n",
    "    \n",
    "    return {\n",
    "        'params': {\n",
    "            'embedding_matrix': embedding_matrix,\n",
    "            'gru_params': gru_params,\n",
    "            'softmax_params': softmax_params,\n",
    "        },\n",
    "        'ops': {\n",
    "            'input_sequence': input_sequence,\n",
    "            'embedded_inputs': embedded_inputs,\n",
    "            'h_states': h_states\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "my_rnn = make_rnn(\n",
    "    vocab_size=50,\n",
    "    embedding_size=6,\n",
    "    hidden_size=3,\n",
    "    batch_size=4, \n",
    "    num_steps=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ops': {'embedded_inputs': <tf.Tensor 'RNN/embedded_inputs:0' shape=(4, 5, 6) dtype=float16>,\n",
       "  'h_states': [<tf.Tensor 'RNN/gru_calculations/add_6:0' shape=(4, 3) dtype=float16>,\n",
       "   <tf.Tensor 'RNN/gru_calculations_1/add_6:0' shape=(4, 3) dtype=float16>,\n",
       "   <tf.Tensor 'RNN/gru_calculations_2/add_6:0' shape=(4, 3) dtype=float16>,\n",
       "   <tf.Tensor 'RNN/gru_calculations_3/add_6:0' shape=(4, 3) dtype=float16>,\n",
       "   <tf.Tensor 'RNN/gru_calculations_4/add_6:0' shape=(4, 3) dtype=float16>],\n",
       "  'input_sequence': <tf.Tensor 'RNN/input_sequence:0' shape=(4, 5) dtype=int32>},\n",
       " 'params': {'embedding_matrix': <tf.Variable 'RNNParams/embedding:0' shape=(50, 6) dtype=float16_ref>,\n",
       "  'gru_params': {'U_h': <tf.Variable 'RNNParams/U_h:0' shape=(3, 3) dtype=float16_ref>,\n",
       "   'U_r': <tf.Variable 'RNNParams/U_r:0' shape=(3, 3) dtype=float16_ref>,\n",
       "   'U_z': <tf.Variable 'RNNParams/U_z:0' shape=(3, 3) dtype=float16_ref>,\n",
       "   'W_h': <tf.Variable 'RNNParams/W_h:0' shape=(6, 3) dtype=float16_ref>,\n",
       "   'W_r': <tf.Variable 'RNNParams/W_r:0' shape=(6, 3) dtype=float16_ref>,\n",
       "   'W_z': <tf.Variable 'RNNParams/W_z:0' shape=(6, 3) dtype=float16_ref>,\n",
       "   'b_h': <tf.Variable 'RNNParams/b_h:0' shape=(3,) dtype=float16_ref>,\n",
       "   'b_r': <tf.Variable 'RNNParams/b_r:0' shape=(3,) dtype=float16_ref>,\n",
       "   'b_z': <tf.Variable 'RNNParams/b_z:0' shape=(3,) dtype=float16_ref>},\n",
       "  'softmax_params': {'W': <tf.Variable 'RNNParams/softmax_w:0' shape=(3, 50) dtype=float16_ref>,\n",
       "   'b': <tf.Variable 'RNNParams/softmax_b:0' shape=(50,) dtype=float16_ref>}}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  6  7  8  9]\n",
      " [ 4  5  6  7 10 11 12 13]\n",
      " [ 8  9 10 11 14 15 16 17]]\n",
      "\n",
      "[[ 0  1  2  3]\n",
      " [ 6  7  8  9]\n",
      " [ 4  5  6  7]\n",
      " [10 11 12 13]\n",
      " [ 8  9 10 11]\n",
      " [14 15 16 17]]\n",
      "\n",
      "[[[ 0  1  2  3]\n",
      "  [ 6  7  8  9]]\n",
      "\n",
      " [[ 4  5  6  7]\n",
      "  [10 11 12 13]]\n",
      "\n",
      " [[ 8  9 10 11]\n",
      "  [14 15 16 17]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In this example, batch_size is 3, hidden_size = 4, num_steps = 2\n",
    "m1 = tf.constant(np.reshape(np.arange(12),(3,4)))\n",
    "m2 = tf.constant(6 + np.reshape(np.arange(12),(3,4)))\n",
    "concatenated_ms = tf.concat([m1, m2], axis=1)\n",
    "skinny_ms = tf.reshape(concatenated_ms, [-1, 4])\n",
    "reshaped_ms = tf.reshape(skinny_ms, [-1, 2, 4])\n",
    "with tf.Session() as sess:\n",
    "    for m in sess.run([\n",
    "        concatenated_ms,\n",
    "        skinny_ms,\n",
    "        reshaped_ms\n",
    "    ]):\n",
    "        print(m)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 2 but is rank 3 for 'MatMul_1' (op: 'MatMul') with input shapes: [2,3,4], [4,2].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda2/envs/py3k/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    653\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    655\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3k/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3k/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shape must be rank 2 but is rank 3 for 'MatMul_1' (op: 'MatMul') with input shapes: [2,3,4], [4,2].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-6a23eebed92f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mm2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/py3k/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m       return gen_math_ops._mat_mul(\n\u001b[0;32m-> 1844\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3k/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_mat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   1287\u001b[0m   \"\"\"\n\u001b[1;32m   1288\u001b[0m   result = _op_def_lib.apply_op(\"MatMul\", a=a, b=b, transpose_a=transpose_a,\n\u001b[0;32m-> 1289\u001b[0;31m                                 transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1290\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3k/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3k/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2630\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2631\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2632\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2633\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2634\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3k/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1909\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m~/anaconda2/envs/py3k/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3k/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    593\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    594\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    596\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3k/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    657\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 2 but is rank 3 for 'MatMul_1' (op: 'MatMul') with input shapes: [2,3,4], [4,2]."
     ]
    }
   ],
   "source": [
    "m1 = tf.constant(np.reshape(np.arange(24),(2,3,4)), dtype=tf.float32)\n",
    "m2 = tf.constant(np.reshape(np.arange(8),(4,2)), dtype=tf.float32)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.matmul(m1, m2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"batcher/x:0\", shape=(4, 3), dtype=int32) Tensor(\"batcher/y:0\", shape=(4, 3), dtype=int32)\n",
      "INFO:tensorflow:Starting standard services.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting standard services.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting queue runners.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting queue runners.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Recording summary at step None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Recording summary at step None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   1   2]\n",
      " [ 50  51  52]\n",
      " [100 101 102]\n",
      " [150 151 152]]\n",
      "[[  1   2   3]\n",
      " [ 51  52  53]\n",
      " [101 102 103]\n",
      " [151 152 153]]\n",
      "[[  3   4   5]\n",
      " [ 53  54  55]\n",
      " [103 104 105]\n",
      " [153 154 155]]\n",
      "[[  4   5   6]\n",
      " [ 54  55  56]\n",
      " [104 105 106]\n",
      " [154 155 156]]\n"
     ]
    }
   ],
   "source": [
    "tf_x, tf_y = ptb_batcher(np.arange(200), 4, 3)\n",
    "print(tf_x, tf_y)sv = tf.train.Supervisor(logdir='logs')\n",
    "with sv.managed_session() as sess:\n",
    "    for i in range(2):\n",
    "        xout, yout = sess.run([tf_x, tf_y])\n",
    "        print(xout)\n",
    "        print(yout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('foo'):\n",
    "    with tf.name_scope('foo')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
