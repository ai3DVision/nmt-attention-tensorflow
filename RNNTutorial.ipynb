{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "np.set_printoptions(precision=4, linewidth=200)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.reader import ptb_raw_data\n",
    "from utils.batcher import ptb_batcher\n",
    "from utils.conditional_scope import cond_name_scope, cond_variable_scope\n",
    "from utils.unrolled_rnn import make_rnn_variables\n",
    "from utils.unrolled_rnn import make_rnn_outputs\n",
    "from utils.unrolled_rnn import make_summary_nodes\n",
    "from utils.unrolled_rnn import make_placeholders\n",
    "from utils.unrolled_rnn import make_train_op\n",
    "from utils.batcher import generate_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, vocab_size = ptb_raw_data('bigdata/simple-examples/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE=64\n",
    "HIDDEN_SIZE=256\n",
    "BATCH_SIZE=32\n",
    "NUM_STEPS=16\n",
    "NUM_EPOCHS_INIT_LR=3\n",
    "NUM_EPOCHS_TOTAL=8\n",
    "INITIAL_LR=5e0\n",
    "LR_DECAY_RATE=0.75\n",
    "MAX_NORM=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "placeholders = make_placeholders(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_steps=NUM_STEPS\n",
    ")\n",
    "rnn_vars = make_rnn_variables(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    initializer_scale=0.2\n",
    ")\n",
    "rnn_outputs = make_rnn_outputs(\n",
    "    input_sequence=placeholders['inputs'],\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_steps=NUM_STEPS,\n",
    "    rnn_variables=rnn_vars\n",
    ")\n",
    "summary_nodes = make_summary_nodes(\n",
    "    targets=placeholders['targets'],\n",
    "    logits=rnn_outputs['logits'],\n",
    ")\n",
    "training_nodes = make_train_op(\n",
    "    summary_nodes['loss'],\n",
    "    placeholders['learning_rate'],\n",
    "    placeholders['max_norm'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 64    loss: 7.442488670349121    gradient norm: 0.6903454065322876     correct words: 38\n",
      "step: 128    loss: 8.22467041015625    gradient norm: 3.464698314666748     correct words: 47\n",
      "step: 192    loss: 7.208061695098877    gradient norm: 2.0766000747680664     correct words: 49\n",
      "step: 256    loss: 6.89780855178833    gradient norm: 0.7592272758483887     correct words: 36\n",
      "step: 320    loss: 6.7141642570495605    gradient norm: 0.8033626675605774     correct words: 30\n",
      "step: 384    loss: 6.479608535766602    gradient norm: 0.5325131416320801     correct words: 46\n",
      "step: 448    loss: 7.2704291343688965    gradient norm: 2.3638126850128174     correct words: 34\n",
      "step: 512    loss: 6.724634170532227    gradient norm: 1.2482047080993652     correct words: 53\n",
      "step: 576    loss: 6.448917865753174    gradient norm: 1.2443323135375977     correct words: 66\n",
      "step: 640    loss: 6.110818862915039    gradient norm: 0.7929134368896484     correct words: 59\n",
      "step: 704    loss: 6.256288051605225    gradient norm: 1.1971123218536377     correct words: 56\n",
      "step: 768    loss: 6.2092366218566895    gradient norm: 0.584235429763794     correct words: 46\n",
      "step: 832    loss: 6.192357063293457    gradient norm: 0.9204073548316956     correct words: 62\n",
      "step: 896    loss: 5.858716011047363    gradient norm: 1.0241202116012573     correct words: 74\n",
      "step: 960    loss: 5.8142595291137695    gradient norm: 0.47888243198394775     correct words: 82\n",
      "step: 1024    loss: 6.38902473449707    gradient norm: 1.1241083145141602     correct words: 69\n",
      "step: 1088    loss: 6.401142597198486    gradient norm: 1.9384373426437378     correct words: 83\n",
      "step: 1152    loss: 6.372989654541016    gradient norm: 1.5344023704528809     correct words: 68\n",
      "step: 1216    loss: 5.834036350250244    gradient norm: 0.6032688021659851     correct words: 67\n",
      "step: 1280    loss: 5.757381916046143    gradient norm: 0.48263534903526306     correct words: 80\n",
      "step: 1344    loss: 5.697516441345215    gradient norm: 0.5140452980995178     correct words: 76\n",
      "step: 1408    loss: 5.867586612701416    gradient norm: 0.4989875555038452     correct words: 73\n",
      "step: 1472    loss: 6.1275787353515625    gradient norm: 0.5843000411987305     correct words: 61\n",
      "step: 1536    loss: 6.257277488708496    gradient norm: 1.2078627347946167     correct words: 72\n",
      "step: 1600    loss: 5.866221904754639    gradient norm: 0.5311838984489441     correct words: 78\n",
      "step: 1664    loss: 5.837521553039551    gradient norm: 0.48891976475715637     correct words: 75\n",
      "step: 1728    loss: 5.945531368255615    gradient norm: 0.5435956120491028     correct words: 66\n",
      "step: 1792    loss: 6.008633136749268    gradient norm: 1.3767081499099731     correct words: 74\n",
      "validation perplexity: 434.722804757\n",
      "test perplexity: 427.188428542\n",
      "step: 64    loss: 5.860420227050781    gradient norm: 0.5315761566162109     correct words: 84\n",
      "step: 128    loss: 5.980352878570557    gradient norm: 1.0356351137161255     correct words: 67\n",
      "step: 192    loss: 6.038645267486572    gradient norm: 1.21652090549469     correct words: 77\n",
      "step: 256    loss: 5.688960075378418    gradient norm: 0.884596586227417     correct words: 76\n",
      "step: 320    loss: 5.477691650390625    gradient norm: 0.5082293748855591     correct words: 92\n",
      "step: 384    loss: 5.709192276000977    gradient norm: 0.5482237935066223     correct words: 73\n",
      "step: 448    loss: 5.793558120727539    gradient norm: 0.506279468536377     correct words: 72\n",
      "step: 512    loss: 5.882582187652588    gradient norm: 1.0764460563659668     correct words: 78\n",
      "step: 576    loss: 5.6156792640686035    gradient norm: 0.8540658950805664     correct words: 85\n",
      "step: 640    loss: 5.452976703643799    gradient norm: 0.6235286593437195     correct words: 77\n",
      "step: 704    loss: 5.614193916320801    gradient norm: 0.5763370990753174     correct words: 84\n",
      "step: 768    loss: 5.843274116516113    gradient norm: 1.5496453046798706     correct words: 75\n",
      "step: 832    loss: 5.487752914428711    gradient norm: 0.6879819631576538     correct words: 88\n",
      "step: 896    loss: 5.3384175300598145    gradient norm: 0.5558551549911499     correct words: 82\n",
      "step: 960    loss: 5.3792290687561035    gradient norm: 0.8721647262573242     correct words: 90\n",
      "step: 1024    loss: 5.814756393432617    gradient norm: 0.969391405582428     correct words: 91\n",
      "step: 1088    loss: 5.770806312561035    gradient norm: 1.470461130142212     correct words: 83\n",
      "step: 1152    loss: 5.784685134887695    gradient norm: 0.5507426261901855     correct words: 79\n",
      "step: 1216    loss: 5.29202127456665    gradient norm: 0.9554407596588135     correct words: 89\n",
      "step: 1280    loss: 5.321589469909668    gradient norm: 0.5315150022506714     correct words: 87\n",
      "step: 1344    loss: 5.430028915405273    gradient norm: 0.7654406428337097     correct words: 84\n",
      "step: 1408    loss: 5.428549289703369    gradient norm: 0.5652914047241211     correct words: 86\n",
      "step: 1472    loss: 5.550809860229492    gradient norm: 0.53911954164505     correct words: 79\n",
      "step: 1536    loss: 5.644240379333496    gradient norm: 0.5556080341339111     correct words: 79\n",
      "step: 1600    loss: 5.517906188964844    gradient norm: 1.123478889465332     correct words: 76\n",
      "step: 1664    loss: 5.503170490264893    gradient norm: 0.534925639629364     correct words: 97\n",
      "step: 1728    loss: 5.509366035461426    gradient norm: 0.48050516843795776     correct words: 98\n",
      "step: 1792    loss: 5.495100975036621    gradient norm: 0.5517528653144836     correct words: 76\n",
      "validation perplexity: 279.569123765\n",
      "test perplexity: 272.765733102\n",
      "step: 64    loss: 5.478744983673096    gradient norm: 0.94155353307724     correct words: 84\n",
      "step: 128    loss: 5.655595302581787    gradient norm: 0.5120548605918884     correct words: 79\n",
      "step: 192    loss: 5.625586032867432    gradient norm: 0.532052755355835     correct words: 79\n",
      "step: 256    loss: 5.446111679077148    gradient norm: 0.8855563998222351     correct words: 79\n",
      "step: 320    loss: 5.229182720184326    gradient norm: 0.5331628322601318     correct words: 102\n",
      "step: 384    loss: 5.3457417488098145    gradient norm: 0.8759870529174805     correct words: 99\n",
      "step: 448    loss: 5.488724708557129    gradient norm: 0.5792832970619202     correct words: 76\n",
      "step: 512    loss: 5.398049831390381    gradient norm: 0.5607340931892395     correct words: 89\n",
      "step: 576    loss: 5.3153557777404785    gradient norm: 0.660382866859436     correct words: 91\n",
      "step: 640    loss: 5.28973388671875    gradient norm: 0.7378504276275635     correct words: 92\n",
      "step: 704    loss: 5.227757930755615    gradient norm: 0.5509445071220398     correct words: 81\n",
      "step: 768    loss: 5.421512603759766    gradient norm: 0.5616359114646912     correct words: 85\n",
      "step: 832    loss: 5.238594055175781    gradient norm: 0.8057500720024109     correct words: 102\n",
      "step: 896    loss: 5.214400291442871    gradient norm: 1.0787423849105835     correct words: 90\n",
      "step: 960    loss: 5.184454917907715    gradient norm: 0.5573963522911072     correct words: 99\n",
      "step: 1024    loss: 5.703730583190918    gradient norm: 1.2076728343963623     correct words: 101\n",
      "step: 1088    loss: 5.243655204772949    gradient norm: 0.7479381561279297     correct words: 107\n",
      "step: 1152    loss: 5.431962013244629    gradient norm: 0.5679309964179993     correct words: 91\n",
      "step: 1216    loss: 5.2493577003479    gradient norm: 0.6871167421340942     correct words: 93\n",
      "step: 1280    loss: 5.067070484161377    gradient norm: 0.5565580725669861     correct words: 103\n",
      "step: 1344    loss: 4.971049785614014    gradient norm: 0.5258103013038635     correct words: 94\n",
      "step: 1408    loss: 5.136452674865723    gradient norm: 0.5709088444709778     correct words: 95\n",
      "step: 1472    loss: 5.114779949188232    gradient norm: 0.5213951468467712     correct words: 106\n",
      "step: 1536    loss: 5.5043864250183105    gradient norm: 1.219087839126587     correct words: 99\n",
      "step: 1600    loss: 5.278901100158691    gradient norm: 0.9077993035316467     correct words: 90\n",
      "step: 1664    loss: 5.268381595611572    gradient norm: 0.5261682271957397     correct words: 104\n",
      "step: 1728    loss: 5.308291435241699    gradient norm: 0.5735716223716736     correct words: 103\n",
      "step: 1792    loss: 5.34539794921875    gradient norm: 1.0377404689788818     correct words: 92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation perplexity: 241.55689653\n",
      "test perplexity: 236.897568478\n",
      "step: 64    loss: 5.1805500984191895    gradient norm: 0.49780505895614624     correct words: 106\n",
      "step: 128    loss: 5.205382347106934    gradient norm: 0.4922258257865906     correct words: 92\n",
      "step: 192    loss: 5.188063621520996    gradient norm: 0.7560275197029114     correct words: 99\n",
      "step: 256    loss: 5.139129161834717    gradient norm: 0.569052517414093     correct words: 93\n",
      "step: 320    loss: 4.870426654815674    gradient norm: 0.57475346326828     correct words: 112\n",
      "step: 384    loss: 4.969931602478027    gradient norm: 0.5993033051490784     correct words: 104\n",
      "step: 448    loss: 5.3567914962768555    gradient norm: 0.9596911668777466     correct words: 89\n",
      "step: 512    loss: 5.09026575088501    gradient norm: 0.5136498212814331     correct words: 101\n",
      "step: 576    loss: 5.0604681968688965    gradient norm: 0.6595025658607483     correct words: 98\n",
      "step: 640    loss: 4.831742286682129    gradient norm: 0.7192752957344055     correct words: 110\n",
      "step: 704    loss: 5.015286445617676    gradient norm: 0.6285732388496399     correct words: 94\n",
      "step: 768    loss: 4.986227035522461    gradient norm: 0.5511857271194458     correct words: 94\n",
      "step: 832    loss: 4.8955278396606445    gradient norm: 0.6436411142349243     correct words: 101\n",
      "step: 896    loss: 4.905025482177734    gradient norm: 0.6008062958717346     correct words: 111\n",
      "step: 960    loss: 4.814784049987793    gradient norm: 0.610809326171875     correct words: 112\n",
      "step: 1024    loss: 5.209280014038086    gradient norm: 0.550982654094696     correct words: 91\n",
      "step: 1088    loss: 4.940758228302002    gradient norm: 0.5178039073944092     correct words: 121\n",
      "step: 1152    loss: 5.044241905212402    gradient norm: 0.5295696258544922     correct words: 112\n",
      "step: 1216    loss: 4.691526412963867    gradient norm: 0.5924543142318726     correct words: 117\n",
      "step: 1280    loss: 4.627573013305664    gradient norm: 0.5375978350639343     correct words: 114\n",
      "step: 1344    loss: 4.809643268585205    gradient norm: 0.6158684492111206     correct words: 98\n",
      "step: 1408    loss: 4.803230285644531    gradient norm: 0.5392258763313293     correct words: 106\n",
      "step: 1472    loss: 4.894542694091797    gradient norm: 0.6521164178848267     correct words: 122\n",
      "step: 1536    loss: 5.099967002868652    gradient norm: 0.8962298035621643     correct words: 103\n",
      "step: 1600    loss: 4.914517879486084    gradient norm: 0.5073897838592529     correct words: 120\n",
      "step: 1664    loss: 4.9200944900512695    gradient norm: 0.5610666275024414     correct words: 114\n",
      "step: 1728    loss: 4.950263977050781    gradient norm: 0.7230020761489868     correct words: 99\n",
      "step: 1792    loss: 4.988376617431641    gradient norm: 0.5832955241203308     correct words: 100\n",
      "validation perplexity: 178.735788208\n",
      "test perplexity: 170.51249191\n",
      "step: 64    loss: 4.92547607421875    gradient norm: 0.5108714699745178     correct words: 118\n",
      "step: 128    loss: 4.9698286056518555    gradient norm: 0.4730277359485626     correct words: 111\n",
      "step: 192    loss: 4.960893630981445    gradient norm: 0.6436469554901123     correct words: 111\n",
      "step: 256    loss: 4.846147060394287    gradient norm: 0.5780336856842041     correct words: 110\n",
      "step: 320    loss: 4.6612229347229    gradient norm: 0.6582384705543518     correct words: 129\n",
      "step: 384    loss: 4.7269606590271    gradient norm: 0.5008306503295898     correct words: 119\n",
      "step: 448    loss: 5.02623176574707    gradient norm: 0.5067404508590698     correct words: 98\n",
      "step: 512    loss: 4.937561511993408    gradient norm: 0.5677011013031006     correct words: 95\n",
      "step: 576    loss: 4.704591751098633    gradient norm: 0.548823356628418     correct words: 130\n",
      "step: 640    loss: 4.621627330780029    gradient norm: 0.5614460110664368     correct words: 119\n",
      "step: 704    loss: 4.591742992401123    gradient norm: 0.5142639875411987     correct words: 115\n",
      "step: 768    loss: 4.800256252288818    gradient norm: 0.569709062576294     correct words: 111\n",
      "step: 832    loss: 4.640188694000244    gradient norm: 0.5500741004943848     correct words: 118\n",
      "step: 896    loss: 4.624672889709473    gradient norm: 0.5673374533653259     correct words: 120\n",
      "step: 960    loss: 4.5837531089782715    gradient norm: 0.554968535900116     correct words: 122\n",
      "step: 1024    loss: 5.063943862915039    gradient norm: 0.9625024795532227     correct words: 110\n",
      "step: 1088    loss: 4.817237377166748    gradient norm: 0.5942949652671814     correct words: 125\n",
      "step: 1152    loss: 4.842308521270752    gradient norm: 0.5516852140426636     correct words: 118\n",
      "step: 1216    loss: 4.493509769439697    gradient norm: 0.5705751180648804     correct words: 127\n",
      "step: 1280    loss: 4.4572248458862305    gradient norm: 0.5846362709999084     correct words: 128\n",
      "step: 1344    loss: 4.587116718292236    gradient norm: 0.5761104226112366     correct words: 110\n",
      "step: 1408    loss: 4.545135498046875    gradient norm: 0.5578161478042603     correct words: 115\n",
      "step: 1472    loss: 4.717045307159424    gradient norm: 0.5672056674957275     correct words: 128\n",
      "step: 1536    loss: 4.835668563842773    gradient norm: 0.6690068244934082     correct words: 115\n",
      "step: 1600    loss: 4.7275848388671875    gradient norm: 0.5737786293029785     correct words: 127\n",
      "step: 1664    loss: 4.720760822296143    gradient norm: 0.8199411034584045     correct words: 133\n",
      "step: 1728    loss: 4.7019944190979    gradient norm: 0.5225491523742676     correct words: 126\n",
      "step: 1792    loss: 4.766472339630127    gradient norm: 0.6252105832099915     correct words: 105\n",
      "validation perplexity: 158.46595374\n",
      "test perplexity: 150.541819283\n",
      "step: 64    loss: 4.743872165679932    gradient norm: 0.5173543691635132     correct words: 117\n",
      "step: 128    loss: 4.924953460693359    gradient norm: 0.7330543994903564     correct words: 114\n",
      "step: 192    loss: 4.774526596069336    gradient norm: 0.5358455181121826     correct words: 117\n",
      "step: 256    loss: 4.655395984649658    gradient norm: 0.5678272247314453     correct words: 125\n",
      "step: 320    loss: 4.53621244430542    gradient norm: 0.5901316404342651     correct words: 136\n",
      "step: 384    loss: 4.5754241943359375    gradient norm: 0.522171139717102     correct words: 125\n",
      "step: 448    loss: 4.8864569664001465    gradient norm: 0.5351030826568604     correct words: 105\n",
      "step: 512    loss: 4.812894344329834    gradient norm: 0.5389245748519897     correct words: 105\n",
      "step: 576    loss: 4.581188678741455    gradient norm: 0.5387788414955139     correct words: 131\n",
      "step: 640    loss: 4.497218608856201    gradient norm: 0.5511329770088196     correct words: 118\n",
      "step: 704    loss: 4.516061782836914    gradient norm: 0.6196441650390625     correct words: 124\n",
      "step: 768    loss: 4.667319297790527    gradient norm: 0.5455745458602905     correct words: 114\n",
      "step: 832    loss: 4.439190864562988    gradient norm: 0.539230227470398     correct words: 129\n",
      "step: 896    loss: 4.491284370422363    gradient norm: 0.5522817969322205     correct words: 134\n",
      "step: 960    loss: 4.450351715087891    gradient norm: 0.5740819573402405     correct words: 131\n",
      "step: 1024    loss: 4.912651062011719    gradient norm: 0.8076270222663879     correct words: 111\n",
      "step: 1088    loss: 4.654593467712402    gradient norm: 0.5595558881759644     correct words: 129\n",
      "step: 1152    loss: 4.702558994293213    gradient norm: 0.5405861139297485     correct words: 125\n",
      "step: 1216    loss: 4.347939491271973    gradient norm: 0.5829231142997742     correct words: 129\n",
      "step: 1280    loss: 4.328967094421387    gradient norm: 0.5913087129592896     correct words: 140\n",
      "step: 1344    loss: 4.430611610412598    gradient norm: 0.5744864344596863     correct words: 125\n",
      "step: 1408    loss: 4.364551544189453    gradient norm: 0.5767131447792053     correct words: 125\n",
      "step: 1472    loss: 4.604037761688232    gradient norm: 0.580495297908783     correct words: 129\n",
      "step: 1536    loss: 4.652124404907227    gradient norm: 0.5504424571990967     correct words: 128\n",
      "step: 1600    loss: 4.608212471008301    gradient norm: 0.5906699895858765     correct words: 135\n",
      "step: 1664    loss: 4.5351080894470215    gradient norm: 0.5407332181930542     correct words: 129\n",
      "step: 1728    loss: 4.580049514770508    gradient norm: 0.540345311164856     correct words: 127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1792    loss: 4.583516597747803    gradient norm: 0.608253002166748     correct words: 113\n",
      "validation perplexity: 149.82412332\n",
      "test perplexity: 142.552388792\n",
      "step: 64    loss: 4.627676486968994    gradient norm: 0.5337792634963989     correct words: 126\n",
      "step: 128    loss: 4.83149528503418    gradient norm: 0.6529021263122559     correct words: 119\n",
      "step: 192    loss: 4.6521220207214355    gradient norm: 0.5601725578308105     correct words: 121\n",
      "step: 256    loss: 4.542318820953369    gradient norm: 0.5724175572395325     correct words: 123\n",
      "step: 320    loss: 4.438754558563232    gradient norm: 0.5586279630661011     correct words: 140\n",
      "step: 384    loss: 4.4756855964660645    gradient norm: 0.5435066819190979     correct words: 133\n",
      "step: 448    loss: 4.813286304473877    gradient norm: 0.6050848364830017     correct words: 105\n",
      "step: 512    loss: 4.719724655151367    gradient norm: 0.5664400458335876     correct words: 115\n",
      "step: 576    loss: 4.490606784820557    gradient norm: 0.5527645945549011     correct words: 136\n",
      "step: 640    loss: 4.405157089233398    gradient norm: 0.5735642313957214     correct words: 121\n",
      "step: 704    loss: 4.393001556396484    gradient norm: 0.5790010690689087     correct words: 127\n",
      "step: 768    loss: 4.577952861785889    gradient norm: 0.5828984975814819     correct words: 117\n",
      "step: 832    loss: 4.31182336807251    gradient norm: 0.5565829873085022     correct words: 139\n",
      "step: 896    loss: 4.401242733001709    gradient norm: 0.5790320634841919     correct words: 140\n",
      "step: 960    loss: 4.3526506423950195    gradient norm: 0.5847110152244568     correct words: 148\n",
      "step: 1024    loss: 4.797882556915283    gradient norm: 0.7066128849983215     correct words: 116\n",
      "step: 1088    loss: 4.558727264404297    gradient norm: 0.5680059790611267     correct words: 136\n",
      "step: 1152    loss: 4.625080108642578    gradient norm: 0.5879576206207275     correct words: 130\n",
      "step: 1216    loss: 4.252859592437744    gradient norm: 0.6096688508987427     correct words: 139\n",
      "step: 1280    loss: 4.223372459411621    gradient norm: 0.5895060896873474     correct words: 141\n",
      "step: 1344    loss: 4.336036682128906    gradient norm: 0.6039876937866211     correct words: 127\n",
      "step: 1408    loss: 4.2366814613342285    gradient norm: 0.5883892774581909     correct words: 133\n",
      "step: 1472    loss: 4.510754108428955    gradient norm: 0.6065654754638672     correct words: 132\n",
      "step: 1536    loss: 4.529748439788818    gradient norm: 0.5735700130462646     correct words: 133\n",
      "step: 1600    loss: 4.513070106506348    gradient norm: 0.603790283203125     correct words: 139\n",
      "step: 1664    loss: 4.430931091308594    gradient norm: 0.5721356272697449     correct words: 138\n",
      "step: 1728    loss: 4.496431827545166    gradient norm: 0.5843027830123901     correct words: 128\n",
      "step: 1792    loss: 4.467813968658447    gradient norm: 0.5951567888259888     correct words: 128\n",
      "validation perplexity: 145.879608431\n",
      "test perplexity: 138.527412836\n",
      "step: 64    loss: 4.5520172119140625    gradient norm: 0.5588244795799255     correct words: 128\n",
      "step: 128    loss: 4.766326427459717    gradient norm: 0.6303932070732117     correct words: 128\n",
      "step: 192    loss: 4.561917304992676    gradient norm: 0.5913724303245544     correct words: 128\n",
      "step: 256    loss: 4.469355583190918    gradient norm: 0.5979997515678406     correct words: 127\n",
      "step: 320    loss: 4.371279716491699    gradient norm: 0.5734084248542786     correct words: 138\n",
      "step: 384    loss: 4.410769939422607    gradient norm: 0.5675230622291565     correct words: 132\n",
      "step: 448    loss: 4.7431440353393555    gradient norm: 0.6118586659431458     correct words: 109\n",
      "step: 512    loss: 4.650568008422852    gradient norm: 0.5821141004562378     correct words: 116\n",
      "step: 576    loss: 4.423974514007568    gradient norm: 0.5800682306289673     correct words: 136\n",
      "step: 640    loss: 4.333015441894531    gradient norm: 0.5970260500907898     correct words: 125\n",
      "step: 704    loss: 4.3231964111328125    gradient norm: 0.5852836966514587     correct words: 123\n",
      "step: 768    loss: 4.5059661865234375    gradient norm: 0.5956576466560364     correct words: 117\n",
      "step: 832    loss: 4.224404335021973    gradient norm: 0.5784677863121033     correct words: 143\n",
      "step: 896    loss: 4.331836223602295    gradient norm: 0.5942034721374512     correct words: 142\n",
      "step: 960    loss: 4.28101921081543    gradient norm: 0.5950786471366882     correct words: 154\n",
      "step: 1024    loss: 4.70908260345459    gradient norm: 0.6578649878501892     correct words: 121\n",
      "step: 1088    loss: 4.48488712310791    gradient norm: 0.5821077227592468     correct words: 142\n",
      "step: 1152    loss: 4.559571266174316    gradient norm: 0.6007267236709595     correct words: 135\n",
      "step: 1216    loss: 4.176119804382324    gradient norm: 0.6185184717178345     correct words: 141\n",
      "step: 1280    loss: 4.155031204223633    gradient norm: 0.618156909942627     correct words: 147\n",
      "step: 1344    loss: 4.2671942710876465    gradient norm: 0.6296067833900452     correct words: 128\n",
      "step: 1408    loss: 4.146625995635986    gradient norm: 0.603441059589386     correct words: 145\n",
      "step: 1472    loss: 4.433923244476318    gradient norm: 0.6236732602119446     correct words: 132\n",
      "step: 1536    loss: 4.444726943969727    gradient norm: 0.5964481234550476     correct words: 136\n",
      "step: 1600    loss: 4.44017219543457    gradient norm: 0.6181687116622925     correct words: 145\n",
      "step: 1664    loss: 4.3642802238464355    gradient norm: 0.6131683588027954     correct words: 140\n",
      "step: 1728    loss: 4.424568176269531    gradient norm: 0.6068069934844971     correct words: 128\n",
      "step: 1792    loss: 4.396097183227539    gradient norm: 0.6082116365432739     correct words: 132\n",
      "validation perplexity: 144.234088837\n",
      "test perplexity: 136.476327938\n"
     ]
    }
   ],
   "source": [
    "training_outputs = {**summary_nodes, **training_nodes}\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Bookkeeping\n",
    "    run_id = time.time()\n",
    "    writer = tf.summary.FileWriter('logs/{0}'.format(run_id), sess.graph)\n",
    "    coord = tf.train.Coordinator()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    learning_rate = INITIAL_LR\n",
    "    max_norm = MAX_NORM\n",
    "    for i in range(NUM_EPOCHS_TOTAL):\n",
    "        if i >= NUM_EPOCHS_INIT_LR:\n",
    "            learning_rate *= LR_DECAY_RATE\n",
    "        for batch_idx, (inputs, targets) in enumerate(generate_epoch(X_train, BATCH_SIZE, NUM_STEPS)):\n",
    "            outputs = sess.run(\n",
    "                training_outputs,\n",
    "                feed_dict={\n",
    "                    placeholders['inputs']: inputs,\n",
    "                    placeholders['targets']: targets,\n",
    "                    placeholders['learning_rate']: learning_rate,\n",
    "                    placeholders['max_norm']: max_norm,\n",
    "                }\n",
    "            )\n",
    "            if (batch_idx % 64 == 63):\n",
    "                print('step: {0}    loss: {1}    gradient norm: {2}     correct words: {3}'.format(\n",
    "                    batch_idx+1,\n",
    "                    outputs['loss'],\n",
    "                    outputs['gradient_global_norm'],\n",
    "                    outputs['num_correct_predictions'],\n",
    "                ))\n",
    "                \n",
    "        total_loss, total_batches = 0, 0\n",
    "        for inputs, targets in generate_epoch(X_val, BATCH_SIZE, NUM_STEPS):\n",
    "            outputs = sess.run(\n",
    "                summary_nodes,\n",
    "                feed_dict={\n",
    "                    placeholders['inputs']: inputs,\n",
    "                    placeholders['targets']: targets\n",
    "                },\n",
    "            )\n",
    "            total_loss += outputs['loss']\n",
    "            total_batches += 1\n",
    "        print('validation perplexity:', np.exp(total_loss / total_batches))\n",
    "        total_loss, total_batches = 0, 0\n",
    "        for inputs, targets in generate_epoch(X_test, BATCH_SIZE, NUM_STEPS):\n",
    "            outputs = sess.run(\n",
    "                summary_nodes,\n",
    "                feed_dict={\n",
    "                    placeholders['inputs']: inputs,\n",
    "                    placeholders['targets']: targets\n",
    "                },\n",
    "            )\n",
    "            total_loss += outputs['loss']\n",
    "            total_batches += 1\n",
    "        print('test perplexity:', np.exp(total_loss / total_batches))\n",
    "\n",
    "    # Bookkeeping        \n",
    "    writer.close()\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
