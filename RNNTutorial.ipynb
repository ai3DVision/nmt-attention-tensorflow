{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "np.set_printoptions(precision=4, linewidth=200)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.reader import ptb_raw_data\n",
    "from utils.batcher import ptb_batcher\n",
    "from utils.conditional_scope import cond_name_scope, cond_variable_scope\n",
    "from utils.unrolled_rnn import make_rnn_variables\n",
    "from utils.unrolled_rnn import make_rnn_outputs\n",
    "from utils.unrolled_rnn import make_summary_nodes\n",
    "from utils.unrolled_rnn import make_placeholders\n",
    "from utils.unrolled_rnn import make_train_op\n",
    "from utils.batcher import generate_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, vocab_size = ptb_raw_data('bigdata/simple-examples/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE=64\n",
    "HIDDEN_SIZE=256\n",
    "BATCH_SIZE=32\n",
    "NUM_STEPS=16\n",
    "NUM_EPOCHS_INIT_LR=3\n",
    "NUM_EPOCHS_TOTAL=8\n",
    "INITIAL_LR=5e0\n",
    "LR_DECAY_RATE=0.75\n",
    "MAX_NORM=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "placeholders = make_placeholders(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_steps=NUM_STEPS\n",
    ")\n",
    "rnn_vars = make_rnn_variables(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    initializer_scale=0.2\n",
    ")\n",
    "rnn_outputs = make_rnn_outputs(\n",
    "    input_sequence=placeholders['inputs'],\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_steps=NUM_STEPS,\n",
    "    rnn_variables=rnn_vars\n",
    ")\n",
    "summary_nodes = make_summary_nodes(\n",
    "    targets=placeholders['targets'],\n",
    "    logits=rnn_outputs['logits'],\n",
    ")\n",
    "training_nodes = make_train_op(\n",
    "    summary_nodes['loss'],\n",
    "    placeholders['learning_rate'],\n",
    "    placeholders['max_norm'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 64    loss: 8.187310218811035    gradient norm: 2.395899772644043     correct words: 22\n",
      "step: 128    loss: 7.125223636627197    gradient norm: 0.4549958407878876     correct words: 13\n",
      "step: 192    loss: 6.89569616317749    gradient norm: 0.691180408000946     correct words: 27\n",
      "step: 256    loss: 7.569201946258545    gradient norm: 2.4837846755981445     correct words: 29\n",
      "step: 320    loss: 6.528324127197266    gradient norm: 0.31351301074028015     correct words: 32\n",
      "step: 384    loss: 7.0342512130737305    gradient norm: 0.7642437219619751     correct words: 33\n",
      "step: 448    loss: 6.986194610595703    gradient norm: 1.3960965871810913     correct words: 37\n",
      "step: 512    loss: 7.007909774780273    gradient norm: 0.4218199849128723     correct words: 27\n",
      "step: 576    loss: 7.155097007751465    gradient norm: 1.537655234336853     correct words: 46\n",
      "step: 640    loss: 6.88791561126709    gradient norm: 1.2366260290145874     correct words: 30\n",
      "step: 704    loss: 6.382382869720459    gradient norm: 1.006024718284607     correct words: 36\n",
      "step: 768    loss: 6.675235748291016    gradient norm: 0.61594557762146     correct words: 36\n",
      "step: 832    loss: 6.581818580627441    gradient norm: 0.3985675871372223     correct words: 43\n",
      "step: 896    loss: 6.460659027099609    gradient norm: 0.5036161541938782     correct words: 49\n",
      "step: 960    loss: 6.221381187438965    gradient norm: 0.8069767951965332     correct words: 46\n",
      "step: 1024    loss: 6.609469413757324    gradient norm: 0.45246943831443787     correct words: 36\n",
      "step: 1088    loss: 6.558779716491699    gradient norm: 0.5896324515342712     correct words: 37\n",
      "step: 1152    loss: 6.424126625061035    gradient norm: 0.4163954555988312     correct words: 44\n",
      "step: 1216    loss: 6.383549690246582    gradient norm: 0.7150219082832336     correct words: 54\n",
      "step: 1280    loss: 6.151719570159912    gradient norm: 0.525231659412384     correct words: 57\n",
      "step: 1344    loss: 6.204443454742432    gradient norm: 0.46291810274124146     correct words: 49\n",
      "step: 1408    loss: 6.083059310913086    gradient norm: 0.562099277973175     correct words: 82\n",
      "step: 1472    loss: 6.535249710083008    gradient norm: 0.45902034640312195     correct words: 50\n",
      "step: 1536    loss: 6.512503623962402    gradient norm: 1.2326409816741943     correct words: 60\n",
      "step: 1600    loss: 6.410205364227295    gradient norm: 1.4243786334991455     correct words: 66\n",
      "step: 1664    loss: 6.836672782897949    gradient norm: 1.8371155261993408     correct words: 51\n",
      "step: 1728    loss: 6.212376117706299    gradient norm: 0.5251247882843018     correct words: 47\n",
      "step: 1792    loss: 6.143176078796387    gradient norm: 0.4856322407722473     correct words: 54\n",
      "validation perplexity: 520.952188552\n",
      "test perplexity: 512.721535693\n",
      "step: 64    loss: 5.900199890136719    gradient norm: 0.3790755867958069     correct words: 57\n",
      "step: 128    loss: 6.1732892990112305    gradient norm: 0.47762733697891235     correct words: 47\n",
      "step: 192    loss: 6.6623759269714355    gradient norm: 1.531837821006775     correct words: 46\n",
      "step: 256    loss: 6.013656139373779    gradient norm: 0.3814697563648224     correct words: 74\n",
      "step: 320    loss: 5.93497371673584    gradient norm: 1.1571240425109863     correct words: 79\n",
      "step: 384    loss: 5.900212287902832    gradient norm: 0.6955763697624207     correct words: 76\n",
      "step: 448    loss: 5.975261688232422    gradient norm: 0.4429533779621124     correct words: 64\n",
      "step: 512    loss: 6.054715156555176    gradient norm: 0.7726055979728699     correct words: 61\n",
      "step: 576    loss: 6.151269912719727    gradient norm: 1.1905092000961304     correct words: 84\n",
      "step: 640    loss: 5.803281307220459    gradient norm: 0.5894586443901062     correct words: 64\n",
      "step: 704    loss: 6.0310492515563965    gradient norm: 0.6412112712860107     correct words: 63\n",
      "step: 768    loss: 6.168375492095947    gradient norm: 1.5810221433639526     correct words: 72\n",
      "step: 832    loss: 5.898758888244629    gradient norm: 0.6740643978118896     correct words: 63\n",
      "step: 896    loss: 5.802351951599121    gradient norm: 1.126259684562683     correct words: 64\n",
      "step: 960    loss: 5.49567174911499    gradient norm: 0.626172661781311     correct words: 82\n",
      "step: 1024    loss: 6.062320709228516    gradient norm: 0.6660913825035095     correct words: 72\n",
      "step: 1088    loss: 5.758285045623779    gradient norm: 0.455925315618515     correct words: 85\n",
      "step: 1152    loss: 5.889163017272949    gradient norm: 0.7618682980537415     correct words: 80\n",
      "step: 1216    loss: 5.471019268035889    gradient norm: 0.7128528952598572     correct words: 76\n",
      "step: 1280    loss: 5.544439792633057    gradient norm: 0.47872158885002136     correct words: 72\n",
      "step: 1344    loss: 5.526177406311035    gradient norm: 0.5609728693962097     correct words: 82\n",
      "step: 1408    loss: 5.649878978729248    gradient norm: 0.7304486036300659     correct words: 71\n",
      "step: 1472    loss: 5.737577438354492    gradient norm: 0.6457093954086304     correct words: 82\n",
      "step: 1536    loss: 5.923830509185791    gradient norm: 1.0204280614852905     correct words: 72\n",
      "step: 1600    loss: 5.827209949493408    gradient norm: 1.0615911483764648     correct words: 82\n",
      "step: 1664    loss: 5.645293712615967    gradient norm: 0.4705953299999237     correct words: 73\n",
      "step: 1728    loss: 5.757227897644043    gradient norm: 0.502043604850769     correct words: 76\n",
      "step: 1792    loss: 5.7428789138793945    gradient norm: 0.5749871730804443     correct words: 70\n",
      "validation perplexity: 324.20486439\n",
      "test perplexity: 324.104055633\n",
      "step: 64    loss: 5.609286785125732    gradient norm: 0.691123366355896     correct words: 70\n",
      "step: 128    loss: 5.994280815124512    gradient norm: 1.3748201131820679     correct words: 72\n",
      "step: 192    loss: 6.021425724029541    gradient norm: 1.278775691986084     correct words: 78\n",
      "step: 256    loss: 5.819911956787109    gradient norm: 0.6023231148719788     correct words: 69\n",
      "step: 320    loss: 5.4426469802856445    gradient norm: 0.9309312701225281     correct words: 79\n",
      "step: 384    loss: 5.614181995391846    gradient norm: 0.8932008147239685     correct words: 75\n",
      "step: 448    loss: 5.671371936798096    gradient norm: 0.5422197580337524     correct words: 68\n",
      "step: 512    loss: 5.687112331390381    gradient norm: 0.8502989411354065     correct words: 84\n",
      "step: 576    loss: 5.856134414672852    gradient norm: 1.338744878768921     correct words: 100\n",
      "step: 640    loss: 5.428253650665283    gradient norm: 0.6280367374420166     correct words: 84\n",
      "step: 704    loss: 5.621740341186523    gradient norm: 0.6189630627632141     correct words: 79\n",
      "step: 768    loss: 5.676044464111328    gradient norm: 0.6324359774589539     correct words: 77\n",
      "step: 832    loss: 5.6549458503723145    gradient norm: 1.1620250940322876     correct words: 89\n",
      "step: 896    loss: 5.458873271942139    gradient norm: 1.1417421102523804     correct words: 82\n",
      "step: 960    loss: 5.580559730529785    gradient norm: 1.1444989442825317     correct words: 82\n",
      "step: 1024    loss: 5.715418815612793    gradient norm: 0.4710909128189087     correct words: 84\n",
      "step: 1088    loss: 5.6849822998046875    gradient norm: 1.2749489545822144     correct words: 96\n",
      "step: 1152    loss: 5.754886627197266    gradient norm: 0.9977432489395142     correct words: 83\n",
      "step: 1216    loss: 5.42280912399292    gradient norm: 0.5701404213905334     correct words: 88\n",
      "step: 1280    loss: 5.307218551635742    gradient norm: 0.539895236492157     correct words: 89\n",
      "step: 1344    loss: 5.092268466949463    gradient norm: 0.5701801180839539     correct words: 101\n",
      "step: 1408    loss: 5.420836925506592    gradient norm: 0.5668159127235413     correct words: 85\n",
      "step: 1472    loss: 5.491864204406738    gradient norm: 0.8486645817756653     correct words: 87\n",
      "step: 1536    loss: 5.609093189239502    gradient norm: 0.5343545079231262     correct words: 68\n",
      "step: 1600    loss: 5.4545063972473145    gradient norm: 0.48580116033554077     correct words: 95\n",
      "step: 1664    loss: 5.415496349334717    gradient norm: 0.4365723729133606     correct words: 101\n",
      "step: 1728    loss: 5.496909141540527    gradient norm: 0.4856038987636566     correct words: 94\n",
      "step: 1792    loss: 5.526956558227539    gradient norm: 0.6182218790054321     correct words: 70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation perplexity: 286.566447698\n",
      "test perplexity: 281.168458673\n",
      "step: 64    loss: 5.3692474365234375    gradient norm: 0.49441781640052795     correct words: 90\n",
      "step: 128    loss: 5.485985279083252    gradient norm: 0.5218965411186218     correct words: 78\n",
      "step: 192    loss: 5.417150974273682    gradient norm: 0.4896920919418335     correct words: 80\n",
      "step: 256    loss: 5.17519998550415    gradient norm: 0.44671931862831116     correct words: 105\n",
      "step: 320    loss: 5.040509223937988    gradient norm: 0.6598420739173889     correct words: 100\n",
      "step: 384    loss: 5.237536430358887    gradient norm: 0.4720807373523712     correct words: 95\n",
      "step: 448    loss: 5.519776821136475    gradient norm: 0.8687852025032043     correct words: 80\n",
      "step: 512    loss: 5.26367712020874    gradient norm: 0.5292115807533264     correct words: 93\n",
      "step: 576    loss: 5.235969066619873    gradient norm: 0.5788769721984863     correct words: 96\n",
      "step: 640    loss: 4.965902805328369    gradient norm: 0.6698638200759888     correct words: 109\n",
      "step: 704    loss: 5.11196231842041    gradient norm: 0.5353366136550903     correct words: 100\n",
      "step: 768    loss: 5.203505992889404    gradient norm: 0.5924315452575684     correct words: 75\n",
      "step: 832    loss: 5.1594648361206055    gradient norm: 0.771876871585846     correct words: 99\n",
      "step: 896    loss: 4.96839714050293    gradient norm: 0.6028886437416077     correct words: 96\n",
      "step: 960    loss: 5.039727210998535    gradient norm: 0.7374631762504578     correct words: 101\n",
      "step: 1024    loss: 5.403504848480225    gradient norm: 0.9224262237548828     correct words: 98\n",
      "step: 1088    loss: 5.319052696228027    gradient norm: 0.6126163601875305     correct words: 90\n",
      "step: 1152    loss: 5.3123369216918945    gradient norm: 0.5279116630554199     correct words: 92\n",
      "step: 1216    loss: 4.856632709503174    gradient norm: 0.5845209360122681     correct words: 116\n",
      "step: 1280    loss: 4.959463596343994    gradient norm: 0.561595618724823     correct words: 100\n",
      "step: 1344    loss: 4.858585834503174    gradient norm: 0.5109158754348755     correct words: 97\n",
      "step: 1408    loss: 5.028119087219238    gradient norm: 0.5096620321273804     correct words: 98\n",
      "step: 1472    loss: 5.097560405731201    gradient norm: 0.5935305953025818     correct words: 109\n",
      "step: 1536    loss: 5.241170883178711    gradient norm: 0.8648868799209595     correct words: 93\n",
      "step: 1600    loss: 5.050534725189209    gradient norm: 0.5018740892410278     correct words: 105\n",
      "step: 1664    loss: 5.101391792297363    gradient norm: 0.49313798546791077     correct words: 102\n",
      "step: 1728    loss: 5.149923324584961    gradient norm: 0.5710538625717163     correct words: 102\n",
      "step: 1792    loss: 5.035090446472168    gradient norm: 0.6320995092391968     correct words: 90\n",
      "validation perplexity: 188.753607622\n",
      "test perplexity: 181.393694053\n",
      "step: 64    loss: 5.115331172943115    gradient norm: 0.6011326909065247     correct words: 96\n",
      "step: 128    loss: 5.15809440612793    gradient norm: 0.4571000635623932     correct words: 97\n",
      "step: 192    loss: 5.058160781860352    gradient norm: 0.5915632843971252     correct words: 98\n",
      "step: 256    loss: 5.013801574707031    gradient norm: 0.542060375213623     correct words: 101\n",
      "step: 320    loss: 4.848875522613525    gradient norm: 0.6242550611495972     correct words: 116\n",
      "step: 384    loss: 4.936463356018066    gradient norm: 0.5072512030601501     correct words: 100\n",
      "step: 448    loss: 5.204039096832275    gradient norm: 0.4741920530796051     correct words: 92\n",
      "step: 512    loss: 5.05256986618042    gradient norm: 0.4870748519897461     correct words: 94\n",
      "step: 576    loss: 4.930393218994141    gradient norm: 0.5431494116783142     correct words: 107\n",
      "step: 640    loss: 4.741967678070068    gradient norm: 0.5765091180801392     correct words: 110\n",
      "step: 704    loss: 4.999367713928223    gradient norm: 0.6221418976783752     correct words: 99\n",
      "step: 768    loss: 4.876925945281982    gradient norm: 0.5131865739822388     correct words: 102\n",
      "step: 832    loss: 4.825282573699951    gradient norm: 0.511171281337738     correct words: 107\n",
      "step: 896    loss: 4.770448684692383    gradient norm: 0.5729326009750366     correct words: 112\n",
      "step: 960    loss: 4.7600860595703125    gradient norm: 0.5533769130706787     correct words: 113\n",
      "step: 1024    loss: 5.165637016296387    gradient norm: 0.8892372250556946     correct words: 102\n",
      "step: 1088    loss: 4.950112819671631    gradient norm: 0.5619204044342041     correct words: 125\n",
      "step: 1152    loss: 5.009809494018555    gradient norm: 0.5316689014434814     correct words: 113\n",
      "step: 1216    loss: 4.764678955078125    gradient norm: 0.6467311978340149     correct words: 113\n",
      "step: 1280    loss: 4.697391986846924    gradient norm: 0.5930764675140381     correct words: 118\n",
      "step: 1344    loss: 4.740082740783691    gradient norm: 0.5602659583091736     correct words: 98\n",
      "step: 1408    loss: 4.746301651000977    gradient norm: 0.5509795546531677     correct words: 106\n",
      "step: 1472    loss: 5.029152870178223    gradient norm: 0.6178004145622253     correct words: 102\n",
      "step: 1536    loss: 4.978465557098389    gradient norm: 0.6891699433326721     correct words: 114\n",
      "step: 1600    loss: 4.807001113891602    gradient norm: 0.5184484124183655     correct words: 121\n",
      "step: 1664    loss: 4.916400909423828    gradient norm: 0.8633982539176941     correct words: 126\n",
      "step: 1728    loss: 4.873361587524414    gradient norm: 0.5613849759101868     correct words: 105\n",
      "step: 1792    loss: 4.953399658203125    gradient norm: 0.6257883310317993     correct words: 91\n",
      "validation perplexity: 165.182063877\n",
      "test perplexity: 158.432863359\n",
      "step: 64    loss: 4.934221267700195    gradient norm: 0.51752108335495     correct words: 107\n",
      "step: 128    loss: 5.082003116607666    gradient norm: 0.6944172978401184     correct words: 104\n",
      "step: 192    loss: 4.8551764488220215    gradient norm: 0.5112566351890564     correct words: 106\n",
      "step: 256    loss: 4.802963733673096    gradient norm: 0.5359891653060913     correct words: 109\n",
      "step: 320    loss: 4.702357769012451    gradient norm: 0.5692248940467834     correct words: 122\n",
      "step: 384    loss: 4.7763190269470215    gradient norm: 0.5061383247375488     correct words: 104\n",
      "step: 448    loss: 5.049944877624512    gradient norm: 0.5251367092132568     correct words: 99\n",
      "step: 512    loss: 4.901011943817139    gradient norm: 0.5259547829627991     correct words: 100\n",
      "step: 576    loss: 4.7558746337890625    gradient norm: 0.5364745259284973     correct words: 127\n",
      "step: 640    loss: 4.58143424987793    gradient norm: 0.5415294170379639     correct words: 112\n",
      "step: 704    loss: 4.784611701965332    gradient norm: 0.6094897985458374     correct words: 112\n",
      "step: 768    loss: 4.733248710632324    gradient norm: 0.5380377173423767     correct words: 110\n",
      "step: 832    loss: 4.59368896484375    gradient norm: 0.5240405797958374     correct words: 119\n",
      "step: 896    loss: 4.626486301422119    gradient norm: 0.5393199324607849     correct words: 127\n",
      "step: 960    loss: 4.602101802825928    gradient norm: 0.5801026225090027     correct words: 127\n",
      "step: 1024    loss: 5.008687973022461    gradient norm: 0.8244472146034241     correct words: 112\n",
      "step: 1088    loss: 4.762424468994141    gradient norm: 0.6003805994987488     correct words: 142\n",
      "step: 1152    loss: 4.823436737060547    gradient norm: 0.5269584059715271     correct words: 121\n",
      "step: 1216    loss: 4.509082794189453    gradient norm: 0.5688867568969727     correct words: 131\n",
      "step: 1280    loss: 4.535374641418457    gradient norm: 0.5858753323554993     correct words: 131\n",
      "step: 1344    loss: 4.580164909362793    gradient norm: 0.5748274922370911     correct words: 110\n",
      "step: 1408    loss: 4.532054424285889    gradient norm: 0.5809136629104614     correct words: 115\n",
      "step: 1472    loss: 4.815546989440918    gradient norm: 0.5938881039619446     correct words: 120\n",
      "step: 1536    loss: 4.7883687019348145    gradient norm: 0.5818284749984741     correct words: 117\n",
      "step: 1600    loss: 4.63809061050415    gradient norm: 0.5772155523300171     correct words: 128\n",
      "step: 1664    loss: 4.684414863586426    gradient norm: 0.5352474451065063     correct words: 132\n",
      "step: 1728    loss: 4.710384368896484    gradient norm: 0.5471880435943604     correct words: 123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1792    loss: 4.744772434234619    gradient norm: 0.6065486073493958     correct words: 103\n",
      "validation perplexity: 151.870021887\n",
      "test perplexity: 145.589816687\n",
      "step: 64    loss: 4.813520431518555    gradient norm: 0.5433322787284851     correct words: 106\n",
      "step: 128    loss: 4.9655280113220215    gradient norm: 0.6324458122253418     correct words: 108\n",
      "step: 192    loss: 4.741883754730225    gradient norm: 0.5546649098396301     correct words: 108\n",
      "step: 256    loss: 4.672945022583008    gradient norm: 0.5560429692268372     correct words: 109\n",
      "step: 320    loss: 4.596774578094482    gradient norm: 0.5572349429130554     correct words: 126\n",
      "step: 384    loss: 4.660979270935059    gradient norm: 0.5391877889633179     correct words: 104\n",
      "step: 448    loss: 4.956696510314941    gradient norm: 0.6182261109352112     correct words: 100\n",
      "step: 512    loss: 4.788901329040527    gradient norm: 0.5687500834465027     correct words: 110\n",
      "step: 576    loss: 4.6385626792907715    gradient norm: 0.560434877872467     correct words: 131\n",
      "step: 640    loss: 4.469681262969971    gradient norm: 0.5686175227165222     correct words: 116\n",
      "step: 704    loss: 4.640711307525635    gradient norm: 0.589898407459259     correct words: 119\n",
      "step: 768    loss: 4.632787227630615    gradient norm: 0.5771556496620178     correct words: 116\n",
      "step: 832    loss: 4.452230453491211    gradient norm: 0.5563138723373413     correct words: 126\n",
      "step: 896    loss: 4.532659530639648    gradient norm: 0.5750850439071655     correct words: 131\n",
      "step: 960    loss: 4.4756083488464355    gradient norm: 0.6031346321105957     correct words: 135\n",
      "step: 1024    loss: 4.8749918937683105    gradient norm: 0.735777735710144     correct words: 112\n",
      "step: 1088    loss: 4.641907691955566    gradient norm: 0.6075937151908875     correct words: 137\n",
      "step: 1152    loss: 4.712850570678711    gradient norm: 0.5851461291313171     correct words: 129\n",
      "step: 1216    loss: 4.407310962677002    gradient norm: 0.6123740673065186     correct words: 142\n",
      "step: 1280    loss: 4.4268269538879395    gradient norm: 0.5960389971733093     correct words: 132\n",
      "step: 1344    loss: 4.483142852783203    gradient norm: 0.6082436442375183     correct words: 113\n",
      "step: 1408    loss: 4.385921478271484    gradient norm: 0.595495343208313     correct words: 124\n",
      "step: 1472    loss: 4.692038536071777    gradient norm: 0.6107507348060608     correct words: 129\n",
      "step: 1536    loss: 4.653481483459473    gradient norm: 0.5787545442581177     correct words: 125\n",
      "step: 1600    loss: 4.517184734344482    gradient norm: 0.6070520877838135     correct words: 134\n",
      "step: 1664    loss: 4.559874057769775    gradient norm: 0.5819675326347351     correct words: 141\n",
      "step: 1728    loss: 4.602553367614746    gradient norm: 0.5879701972007751     correct words: 126\n",
      "step: 1792    loss: 4.612978935241699    gradient norm: 0.6021985411643982     correct words: 110\n",
      "validation perplexity: 145.51251366\n",
      "test perplexity: 139.320293649\n",
      "step: 64    loss: 4.736837863922119    gradient norm: 0.5790305733680725     correct words: 113\n",
      "step: 128    loss: 4.872786521911621    gradient norm: 0.626976490020752     correct words: 112\n",
      "step: 192    loss: 4.661375999450684    gradient norm: 0.6002010107040405     correct words: 114\n",
      "step: 256    loss: 4.592913627624512    gradient norm: 0.603674054145813     correct words: 114\n",
      "step: 320    loss: 4.521299362182617    gradient norm: 0.5853250622749329     correct words: 132\n",
      "step: 384    loss: 4.577230453491211    gradient norm: 0.5716874003410339     correct words: 113\n",
      "step: 448    loss: 4.869842529296875    gradient norm: 0.6278547048568726     correct words: 99\n",
      "step: 512    loss: 4.701801300048828    gradient norm: 0.5922449231147766     correct words: 113\n",
      "step: 576    loss: 4.5544114112854    gradient norm: 0.5947263836860657     correct words: 136\n",
      "step: 640    loss: 4.387503147125244    gradient norm: 0.6012528538703918     correct words: 119\n",
      "step: 704    loss: 4.55016565322876    gradient norm: 0.6119981408119202     correct words: 122\n",
      "step: 768    loss: 4.55271577835083    gradient norm: 0.6084747314453125     correct words: 116\n",
      "step: 832    loss: 4.351195812225342    gradient norm: 0.5866744518280029     correct words: 133\n",
      "step: 896    loss: 4.46043586730957    gradient norm: 0.6028802990913391     correct words: 131\n",
      "step: 960    loss: 4.384603977203369    gradient norm: 0.6219353675842285     correct words: 140\n",
      "step: 1024    loss: 4.7744526863098145    gradient norm: 0.6878785490989685     correct words: 118\n",
      "step: 1088    loss: 4.544592380523682    gradient norm: 0.603075385093689     correct words: 134\n",
      "step: 1152    loss: 4.63026762008667    gradient norm: 0.6146963238716125     correct words: 134\n",
      "step: 1216    loss: 4.329943656921387    gradient norm: 0.6377617120742798     correct words: 143\n",
      "step: 1280    loss: 4.3545427322387695    gradient norm: 0.6308341026306152     correct words: 139\n",
      "step: 1344    loss: 4.409040451049805    gradient norm: 0.6338954567909241     correct words: 118\n",
      "step: 1408    loss: 4.286649703979492    gradient norm: 0.6164431571960449     correct words: 133\n",
      "step: 1472    loss: 4.602105140686035    gradient norm: 0.6389774680137634     correct words: 136\n",
      "step: 1536    loss: 4.556015968322754    gradient norm: 0.6069521307945251     correct words: 133\n",
      "step: 1600    loss: 4.4181365966796875    gradient norm: 0.6289494037628174     correct words: 139\n",
      "step: 1664    loss: 4.479238986968994    gradient norm: 0.6421722769737244     correct words: 139\n",
      "step: 1728    loss: 4.5194878578186035    gradient norm: 0.6267040371894836     correct words: 128\n",
      "step: 1792    loss: 4.523303985595703    gradient norm: 0.62296062707901     correct words: 115\n",
      "validation perplexity: 142.449602784\n",
      "test perplexity: 135.977152467\n"
     ]
    }
   ],
   "source": [
    "training_outputs = {**summary_nodes, **training_nodes}\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Bookkeeping\n",
    "    run_id = time.time()\n",
    "    writer = tf.summary.FileWriter('logs/{0}'.format(run_id), sess.graph)\n",
    "    coord = tf.train.Coordinator()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    learning_rate = INITIAL_LR\n",
    "    max_norm = MAX_NORM\n",
    "    for i in range(NUM_EPOCHS_TOTAL):\n",
    "        if i >= NUM_EPOCHS_INIT_LR:\n",
    "            learning_rate *= LR_DECAY_RATE\n",
    "        for batch_idx, (inputs, targets) in enumerate(generate_epoch(X_train, BATCH_SIZE, NUM_STEPS)):\n",
    "            outputs = sess.run(\n",
    "                training_outputs,\n",
    "                feed_dict={\n",
    "                    placeholders['inputs']: inputs,\n",
    "                    placeholders['targets']: targets,\n",
    "                    placeholders['learning_rate']: learning_rate,\n",
    "                    placeholders['max_norm']: max_norm,\n",
    "                }\n",
    "            )\n",
    "            if (batch_idx % 64 == 63):\n",
    "                print('step: {0}    loss: {1}    gradient norm: {2}     correct words: {3}'.format(\n",
    "                    batch_idx+1,\n",
    "                    outputs['loss'],\n",
    "                    outputs['gradient_global_norm'],\n",
    "                    outputs['num_correct_predictions'],\n",
    "                ))\n",
    "                \n",
    "        total_loss, total_batches = 0, 0\n",
    "        for inputs, targets in generate_epoch(X_val, BATCH_SIZE, NUM_STEPS):\n",
    "            outputs = sess.run(\n",
    "                summary_nodes,\n",
    "                feed_dict={\n",
    "                    placeholders['inputs']: inputs,\n",
    "                    placeholders['targets']: targets\n",
    "                },\n",
    "            )\n",
    "            total_loss += outputs['loss']\n",
    "            total_batches += 1\n",
    "        print('validation perplexity:', np.exp(total_loss / total_batches))\n",
    "        total_loss, total_batches = 0, 0\n",
    "        for inputs, targets in generate_epoch(X_test, BATCH_SIZE, NUM_STEPS):\n",
    "            outputs = sess.run(\n",
    "                summary_nodes,\n",
    "                feed_dict={\n",
    "                    placeholders['inputs']: inputs,\n",
    "                    placeholders['targets']: targets\n",
    "                },\n",
    "            )\n",
    "            total_loss += outputs['loss']\n",
    "            total_batches += 1\n",
    "        print('test perplexity:', np.exp(total_loss / total_batches))\n",
    "\n",
    "    # Bookkeeping        \n",
    "    writer.close()\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
